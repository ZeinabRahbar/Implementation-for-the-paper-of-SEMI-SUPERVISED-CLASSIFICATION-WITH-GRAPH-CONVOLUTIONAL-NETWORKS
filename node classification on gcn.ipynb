{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbEZj2GfdPur",
    "outputId": "1606f668-223b-44a3-a50a-bf61a8360361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "PgS1xFNipZMr"
   },
   "outputs": [],
   "source": [
    "#on cora graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQ4pfr1HjvcD",
    "outputId": "90cf66e3-1f51-4de7-9ce5-cd9fbd37dd30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.8925\n",
      "Epoch [20/100], Loss: 1.7219\n",
      "Epoch [30/100], Loss: 1.4600\n",
      "Epoch [40/100], Loss: 1.2850\n",
      "Epoch [50/100], Loss: 1.2157\n",
      "Epoch [60/100], Loss: 1.1892\n",
      "Epoch [70/100], Loss: 1.1768\n",
      "Epoch [80/100], Loss: 1.1723\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1691\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8943\n",
      "Epoch [20/100], Loss: 1.7187\n",
      "Epoch [30/100], Loss: 1.4464\n",
      "Epoch [40/100], Loss: 1.2762\n",
      "Epoch [50/100], Loss: 1.2126\n",
      "Epoch [60/100], Loss: 1.1892\n",
      "Epoch [70/100], Loss: 1.1809\n",
      "Epoch [80/100], Loss: 1.1749\n",
      "Epoch [90/100], Loss: 1.1707\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8968\n",
      "Epoch [20/100], Loss: 1.7319\n",
      "Epoch [30/100], Loss: 1.4632\n",
      "Epoch [40/100], Loss: 1.2857\n",
      "Epoch [50/100], Loss: 1.2162\n",
      "Epoch [60/100], Loss: 1.1880\n",
      "Epoch [70/100], Loss: 1.1766\n",
      "Epoch [80/100], Loss: 1.1723\n",
      "Epoch [90/100], Loss: 1.1702\n",
      "Epoch [100/100], Loss: 1.1691\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/100], Loss: 1.8918\n",
      "Epoch [20/100], Loss: 1.7242\n",
      "Epoch [30/100], Loss: 1.4594\n",
      "Epoch [40/100], Loss: 1.2852\n",
      "Epoch [50/100], Loss: 1.2167\n",
      "Epoch [60/100], Loss: 1.1911\n",
      "Epoch [70/100], Loss: 1.1818\n",
      "Epoch [80/100], Loss: 1.1767\n",
      "Epoch [90/100], Loss: 1.1716\n",
      "Epoch [100/100], Loss: 1.1695\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8932\n",
      "Epoch [20/100], Loss: 1.7198\n",
      "Epoch [30/100], Loss: 1.4483\n",
      "Epoch [40/100], Loss: 1.2792\n",
      "Epoch [50/100], Loss: 1.2142\n",
      "Epoch [60/100], Loss: 1.1898\n",
      "Epoch [70/100], Loss: 1.1789\n",
      "Epoch [80/100], Loss: 1.1728\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1691\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8934\n",
      "Epoch [20/100], Loss: 1.7184\n",
      "Epoch [30/100], Loss: 1.4486\n",
      "Epoch [40/100], Loss: 1.2800\n",
      "Epoch [50/100], Loss: 1.2155\n",
      "Epoch [60/100], Loss: 1.1911\n",
      "Epoch [70/100], Loss: 1.1804\n",
      "Epoch [80/100], Loss: 1.1732\n",
      "Epoch [90/100], Loss: 1.1706\n",
      "Epoch [100/100], Loss: 1.1693\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8952\n",
      "Epoch [20/100], Loss: 1.7245\n",
      "Epoch [30/100], Loss: 1.4493\n",
      "Epoch [40/100], Loss: 1.2804\n",
      "Epoch [50/100], Loss: 1.2168\n",
      "Epoch [60/100], Loss: 1.1902\n",
      "Epoch [70/100], Loss: 1.1771\n",
      "Epoch [80/100], Loss: 1.1724\n",
      "Epoch [90/100], Loss: 1.1704\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8938\n",
      "Epoch [20/100], Loss: 1.7220\n",
      "Epoch [30/100], Loss: 1.4515\n",
      "Epoch [40/100], Loss: 1.2803\n",
      "Epoch [50/100], Loss: 1.2153\n",
      "Epoch [60/100], Loss: 1.1911\n",
      "Epoch [70/100], Loss: 1.1823\n",
      "Epoch [80/100], Loss: 1.1788\n",
      "Epoch [90/100], Loss: 1.1771\n",
      "Epoch [100/100], Loss: 1.1761\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8956\n",
      "Epoch [20/100], Loss: 1.7282\n",
      "Epoch [30/100], Loss: 1.4669\n",
      "Epoch [40/100], Loss: 1.2897\n",
      "Epoch [50/100], Loss: 1.2190\n",
      "Epoch [60/100], Loss: 1.1896\n",
      "Epoch [70/100], Loss: 1.1772\n",
      "Epoch [80/100], Loss: 1.1727\n",
      "Epoch [90/100], Loss: 1.1705\n",
      "Epoch [100/100], Loss: 1.1693\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8990\n",
      "Epoch [20/100], Loss: 1.7374\n",
      "Epoch [30/100], Loss: 1.4666\n",
      "Epoch [40/100], Loss: 1.2863\n",
      "Epoch [50/100], Loss: 1.2175\n",
      "Epoch [60/100], Loss: 1.1917\n",
      "Epoch [70/100], Loss: 1.1800\n",
      "Epoch [80/100], Loss: 1.1735\n",
      "Epoch [90/100], Loss: 1.1709\n",
      "Epoch [100/100], Loss: 1.1695\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8930\n",
      "Epoch [20/100], Loss: 1.7154\n",
      "Epoch [30/100], Loss: 1.4451\n",
      "Epoch [40/100], Loss: 1.2790\n",
      "Epoch [50/100], Loss: 1.2155\n",
      "Epoch [60/100], Loss: 1.1903\n",
      "Epoch [70/100], Loss: 1.1777\n",
      "Epoch [80/100], Loss: 1.1726\n",
      "Epoch [90/100], Loss: 1.1704\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8961\n",
      "Epoch [20/100], Loss: 1.7260\n",
      "Epoch [30/100], Loss: 1.4505\n",
      "Epoch [40/100], Loss: 1.2769\n",
      "Epoch [50/100], Loss: 1.2125\n",
      "Epoch [60/100], Loss: 1.1895\n",
      "Epoch [70/100], Loss: 1.1812\n",
      "Epoch [80/100], Loss: 1.1750\n",
      "Epoch [90/100], Loss: 1.1709\n",
      "Epoch [100/100], Loss: 1.1693\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8956\n",
      "Epoch [20/100], Loss: 1.7219\n",
      "Epoch [30/100], Loss: 1.4453\n",
      "Epoch [40/100], Loss: 1.2776\n",
      "Epoch [50/100], Loss: 1.2143\n",
      "Epoch [60/100], Loss: 1.1883\n",
      "Epoch [70/100], Loss: 1.1767\n",
      "Epoch [80/100], Loss: 1.1723\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1691\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8953\n",
      "Epoch [20/100], Loss: 1.7238\n",
      "Epoch [30/100], Loss: 1.4543\n",
      "Epoch [40/100], Loss: 1.2820\n",
      "Epoch [50/100], Loss: 1.2159\n",
      "Epoch [60/100], Loss: 1.1909\n",
      "Epoch [70/100], Loss: 1.1820\n",
      "Epoch [80/100], Loss: 1.1777\n",
      "Epoch [90/100], Loss: 1.1721\n",
      "Epoch [100/100], Loss: 1.1697\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8960\n",
      "Epoch [20/100], Loss: 1.7269\n",
      "Epoch [30/100], Loss: 1.4542\n",
      "Epoch [40/100], Loss: 1.2820\n",
      "Epoch [50/100], Loss: 1.2160\n",
      "Epoch [60/100], Loss: 1.1909\n",
      "Epoch [70/100], Loss: 1.1794\n",
      "Epoch [80/100], Loss: 1.1731\n",
      "Epoch [90/100], Loss: 1.1706\n",
      "Epoch [100/100], Loss: 1.1693\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8982\n",
      "Epoch [20/100], Loss: 1.7334\n",
      "Epoch [30/100], Loss: 1.4646\n",
      "Epoch [40/100], Loss: 1.2867\n",
      "Epoch [50/100], Loss: 1.2171\n",
      "Epoch [60/100], Loss: 1.1905\n",
      "Epoch [70/100], Loss: 1.1777\n",
      "Epoch [80/100], Loss: 1.1725\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.9034\n",
      "Epoch [20/100], Loss: 1.7508\n",
      "Epoch [30/100], Loss: 1.4796\n",
      "Epoch [40/100], Loss: 1.2922\n",
      "Epoch [50/100], Loss: 1.2202\n",
      "Epoch [60/100], Loss: 1.1931\n",
      "Epoch [70/100], Loss: 1.1828\n",
      "Epoch [80/100], Loss: 1.1790\n",
      "Epoch [90/100], Loss: 1.1770\n",
      "Epoch [100/100], Loss: 1.1737\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/100], Loss: 1.8874\n",
      "Epoch [20/100], Loss: 1.7054\n",
      "Epoch [30/100], Loss: 1.4487\n",
      "Epoch [40/100], Loss: 1.2808\n",
      "Epoch [50/100], Loss: 1.2152\n",
      "Epoch [60/100], Loss: 1.1890\n",
      "Epoch [70/100], Loss: 1.1770\n",
      "Epoch [80/100], Loss: 1.1726\n",
      "Epoch [90/100], Loss: 1.1705\n",
      "Epoch [100/100], Loss: 1.1693\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8975\n",
      "Epoch [20/100], Loss: 1.7327\n",
      "Epoch [30/100], Loss: 1.4703\n",
      "Epoch [40/100], Loss: 1.2916\n",
      "Epoch [50/100], Loss: 1.2195\n",
      "Epoch [60/100], Loss: 1.1924\n",
      "Epoch [70/100], Loss: 1.1802\n",
      "Epoch [80/100], Loss: 1.1735\n",
      "Epoch [90/100], Loss: 1.1708\n",
      "Epoch [100/100], Loss: 1.1695\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8937\n",
      "Epoch [20/100], Loss: 1.7187\n",
      "Epoch [30/100], Loss: 1.4586\n",
      "Epoch [40/100], Loss: 1.2869\n",
      "Epoch [50/100], Loss: 1.2175\n",
      "Epoch [60/100], Loss: 1.1908\n",
      "Epoch [70/100], Loss: 1.1780\n",
      "Epoch [80/100], Loss: 1.1728\n",
      "Epoch [90/100], Loss: 1.1706\n",
      "Epoch [100/100], Loss: 1.1693\n",
      "Testing Time: 0.01 seconds\n",
      "Mean Training Time: 2.62 seconds\n",
      "Mean Test Accuracy: 0.7901\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.softmax(self.conv2(x, edge_index), dim=-1)\n",
    "        return x\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 64, num_classes)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQsjC-0cgY7e",
    "outputId": "c33fff87-6f0e-4671-c5da-58248ca607b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0152\n",
      "Epoch [20/100], Loss: 0.8385\n",
      "Epoch [30/100], Loss: 0.6845\n",
      "Epoch [40/100], Loss: 0.6122\n",
      "Epoch [50/100], Loss: 0.5855\n",
      "Epoch [60/100], Loss: 0.5737\n",
      "Epoch [70/100], Loss: 0.5629\n",
      "Epoch [80/100], Loss: 0.5583\n",
      "Epoch [90/100], Loss: 0.5559\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.0093\n",
      "Epoch [20/100], Loss: 0.8384\n",
      "Epoch [30/100], Loss: 0.6913\n",
      "Epoch [40/100], Loss: 0.6174\n",
      "Epoch [50/100], Loss: 0.5884\n",
      "Epoch [60/100], Loss: 0.5779\n",
      "Epoch [70/100], Loss: 0.5723\n",
      "Epoch [80/100], Loss: 0.5621\n",
      "Epoch [90/100], Loss: 0.5579\n",
      "Epoch [100/100], Loss: 0.5559\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.0016\n",
      "Epoch [20/100], Loss: 0.8220\n",
      "Epoch [30/100], Loss: 0.6775\n",
      "Epoch [40/100], Loss: 0.6112\n",
      "Epoch [50/100], Loss: 0.5855\n",
      "Epoch [60/100], Loss: 0.5713\n",
      "Epoch [70/100], Loss: 0.5621\n",
      "Epoch [80/100], Loss: 0.5581\n",
      "Epoch [90/100], Loss: 0.5560\n",
      "Epoch [100/100], Loss: 0.5550\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.0092\n",
      "Epoch [20/100], Loss: 0.8356\n",
      "Epoch [30/100], Loss: 0.6872\n",
      "Epoch [40/100], Loss: 0.6149\n",
      "Epoch [50/100], Loss: 0.5852\n",
      "Epoch [60/100], Loss: 0.5685\n",
      "Epoch [70/100], Loss: 0.5611\n",
      "Epoch [80/100], Loss: 0.5575\n",
      "Epoch [90/100], Loss: 0.5558\n",
      "Epoch [100/100], Loss: 0.5549\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.0191\n",
      "Epoch [20/100], Loss: 0.8524\n",
      "Epoch [30/100], Loss: 0.6989\n",
      "Epoch [40/100], Loss: 0.6198\n",
      "Epoch [50/100], Loss: 0.5890\n",
      "Epoch [60/100], Loss: 0.5777\n",
      "Epoch [70/100], Loss: 0.5693\n",
      "Epoch [80/100], Loss: 0.5602\n",
      "Epoch [90/100], Loss: 0.5571\n",
      "Epoch [100/100], Loss: 0.5552\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.0118\n",
      "Epoch [20/100], Loss: 0.8416\n",
      "Epoch [30/100], Loss: 0.6934\n",
      "Epoch [40/100], Loss: 0.6195\n",
      "Epoch [50/100], Loss: 0.5902\n",
      "Epoch [60/100], Loss: 0.5787\n",
      "Epoch [70/100], Loss: 0.5718\n",
      "Epoch [80/100], Loss: 0.5618\n",
      "Epoch [90/100], Loss: 0.5579\n",
      "Epoch [100/100], Loss: 0.5557\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.0206\n",
      "Epoch [20/100], Loss: 0.8499\n",
      "Epoch [30/100], Loss: 0.6951\n",
      "Epoch [40/100], Loss: 0.6186\n",
      "Epoch [50/100], Loss: 0.5888\n",
      "Epoch [60/100], Loss: 0.5778\n",
      "Epoch [70/100], Loss: 0.5690\n",
      "Epoch [80/100], Loss: 0.5605\n",
      "Epoch [90/100], Loss: 0.5574\n",
      "Epoch [100/100], Loss: 0.5555\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.0159\n",
      "Epoch [20/100], Loss: 0.8447\n",
      "Epoch [30/100], Loss: 0.6940\n",
      "Epoch [40/100], Loss: 0.6184\n",
      "Epoch [50/100], Loss: 0.5881\n",
      "Epoch [60/100], Loss: 0.5708\n",
      "Epoch [70/100], Loss: 0.5620\n",
      "Epoch [80/100], Loss: 0.5579\n",
      "Epoch [90/100], Loss: 0.5560\n",
      "Epoch [100/100], Loss: 0.5550\n",
      "Testing Time: 0.06 seconds\n",
      "Epoch [10/100], Loss: 1.0127\n",
      "Epoch [20/100], Loss: 0.8423\n",
      "Epoch [30/100], Loss: 0.6926\n",
      "Epoch [40/100], Loss: 0.6172\n",
      "Epoch [50/100], Loss: 0.5862\n",
      "Epoch [60/100], Loss: 0.5687\n",
      "Epoch [70/100], Loss: 0.5611\n",
      "Epoch [80/100], Loss: 0.5574\n",
      "Epoch [90/100], Loss: 0.5558\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.0217\n",
      "Epoch [20/100], Loss: 0.8577\n",
      "Epoch [30/100], Loss: 0.7052\n",
      "Epoch [40/100], Loss: 0.6255\n",
      "Epoch [50/100], Loss: 0.5915\n",
      "Epoch [60/100], Loss: 0.5734\n",
      "Epoch [70/100], Loss: 0.5638\n",
      "Epoch [80/100], Loss: 0.5592\n",
      "Epoch [90/100], Loss: 0.5569\n",
      "Epoch [100/100], Loss: 0.5556\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.0113\n",
      "Epoch [20/100], Loss: 0.8400\n",
      "Epoch [30/100], Loss: 0.6908\n",
      "Epoch [40/100], Loss: 0.6167\n",
      "Epoch [50/100], Loss: 0.5865\n",
      "Epoch [60/100], Loss: 0.5694\n",
      "Epoch [70/100], Loss: 0.5616\n",
      "Epoch [80/100], Loss: 0.5578\n",
      "Epoch [90/100], Loss: 0.5560\n",
      "Epoch [100/100], Loss: 0.5550\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.0144\n",
      "Epoch [20/100], Loss: 0.8396\n",
      "Epoch [30/100], Loss: 0.6885\n",
      "Epoch [40/100], Loss: 0.6154\n",
      "Epoch [50/100], Loss: 0.5862\n",
      "Epoch [60/100], Loss: 0.5692\n",
      "Epoch [70/100], Loss: 0.5614\n",
      "Epoch [80/100], Loss: 0.5575\n",
      "Epoch [90/100], Loss: 0.5558\n",
      "Epoch [100/100], Loss: 0.5549\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.0189\n",
      "Epoch [20/100], Loss: 0.8469\n",
      "Epoch [30/100], Loss: 0.6922\n",
      "Epoch [40/100], Loss: 0.6166\n",
      "Epoch [50/100], Loss: 0.5877\n",
      "Epoch [60/100], Loss: 0.5727\n",
      "Epoch [70/100], Loss: 0.5627\n",
      "Epoch [80/100], Loss: 0.5583\n",
      "Epoch [90/100], Loss: 0.5561\n",
      "Epoch [100/100], Loss: 0.5550\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.0164\n",
      "Epoch [20/100], Loss: 0.8409\n",
      "Epoch [30/100], Loss: 0.6885\n",
      "Epoch [40/100], Loss: 0.6152\n",
      "Epoch [50/100], Loss: 0.5872\n",
      "Epoch [60/100], Loss: 0.5758\n",
      "Epoch [70/100], Loss: 0.5643\n",
      "Epoch [80/100], Loss: 0.5591\n",
      "Epoch [90/100], Loss: 0.5565\n",
      "Epoch [100/100], Loss: 0.5552\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.0092\n",
      "Epoch [20/100], Loss: 0.8356\n",
      "Epoch [30/100], Loss: 0.6869\n",
      "Epoch [40/100], Loss: 0.6134\n",
      "Epoch [50/100], Loss: 0.5837\n",
      "Epoch [60/100], Loss: 0.5670\n",
      "Epoch [70/100], Loss: 0.5601\n",
      "Epoch [80/100], Loss: 0.5569\n",
      "Epoch [90/100], Loss: 0.5554\n",
      "Epoch [100/100], Loss: 0.5546\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.0155\n",
      "Epoch [20/100], Loss: 0.8448\n",
      "Epoch [30/100], Loss: 0.6935\n",
      "Epoch [40/100], Loss: 0.6182\n",
      "Epoch [50/100], Loss: 0.5887\n",
      "Epoch [60/100], Loss: 0.5775\n",
      "Epoch [70/100], Loss: 0.5669\n",
      "Epoch [80/100], Loss: 0.5599\n",
      "Epoch [90/100], Loss: 0.5569\n",
      "Epoch [100/100], Loss: 0.5553\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.0152\n",
      "Epoch [20/100], Loss: 0.8602\n",
      "Epoch [30/100], Loss: 0.7229\n",
      "Epoch [40/100], Loss: 0.6365\n",
      "Epoch [50/100], Loss: 0.5960\n",
      "Epoch [60/100], Loss: 0.5734\n",
      "Epoch [70/100], Loss: 0.5635\n",
      "Epoch [80/100], Loss: 0.5586\n",
      "Epoch [90/100], Loss: 0.5565\n",
      "Epoch [100/100], Loss: 0.5553\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.0112\n",
      "Epoch [20/100], Loss: 0.8363\n",
      "Epoch [30/100], Loss: 0.6858\n",
      "Epoch [40/100], Loss: 0.6141\n",
      "Epoch [50/100], Loss: 0.5865\n",
      "Epoch [60/100], Loss: 0.5729\n",
      "Epoch [70/100], Loss: 0.5627\n",
      "Epoch [80/100], Loss: 0.5584\n",
      "Epoch [90/100], Loss: 0.5561\n",
      "Epoch [100/100], Loss: 0.5549\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.0130\n",
      "Epoch [20/100], Loss: 0.8388\n",
      "Epoch [30/100], Loss: 0.6873\n",
      "Epoch [40/100], Loss: 0.6146\n",
      "Epoch [50/100], Loss: 0.5873\n",
      "Epoch [60/100], Loss: 0.5774\n",
      "Epoch [70/100], Loss: 0.5733\n",
      "Epoch [80/100], Loss: 0.5662\n",
      "Epoch [90/100], Loss: 0.5585\n",
      "Epoch [100/100], Loss: 0.5560\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.0183\n",
      "Epoch [20/100], Loss: 0.8660\n",
      "Epoch [30/100], Loss: 0.7238\n",
      "Epoch [40/100], Loss: 0.6352\n",
      "Epoch [50/100], Loss: 0.5905\n",
      "Epoch [60/100], Loss: 0.5704\n",
      "Epoch [70/100], Loss: 0.5618\n",
      "Epoch [80/100], Loss: 0.5582\n",
      "Epoch [90/100], Loss: 0.5563\n",
      "Epoch [100/100], Loss: 0.5553\n",
      "Testing Time: 0.05 seconds\n",
      "Mean Training Time: 9.16 seconds\n",
      "Mean Test Accuracy: 0.7614\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.softmax(self.conv2(x, edge_index), dim=-1)\n",
    "        return x\n",
    "\n",
    "# Load the PubMed dataset\n",
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 64, num_classes)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONueYICdgZVG",
    "outputId": "8a12e7a9-186f-4111-d200-41547d045774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.7287\n",
      "Epoch [20/100], Loss: 1.5275\n",
      "Epoch [30/100], Loss: 1.2582\n",
      "Epoch [40/100], Loss: 1.1257\n",
      "Epoch [50/100], Loss: 1.0796\n",
      "Epoch [60/100], Loss: 1.0606\n",
      "Epoch [70/100], Loss: 1.0530\n",
      "Epoch [80/100], Loss: 1.0496\n",
      "Epoch [90/100], Loss: 1.0480\n",
      "Epoch [100/100], Loss: 1.0470\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7343\n",
      "Epoch [20/100], Loss: 1.5381\n",
      "Epoch [30/100], Loss: 1.2676\n",
      "Epoch [40/100], Loss: 1.1272\n",
      "Epoch [50/100], Loss: 1.0794\n",
      "Epoch [60/100], Loss: 1.0607\n",
      "Epoch [70/100], Loss: 1.0529\n",
      "Epoch [80/100], Loss: 1.0495\n",
      "Epoch [90/100], Loss: 1.0478\n",
      "Epoch [100/100], Loss: 1.0469\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7336\n",
      "Epoch [20/100], Loss: 1.5452\n",
      "Epoch [30/100], Loss: 1.2731\n",
      "Epoch [40/100], Loss: 1.1301\n",
      "Epoch [50/100], Loss: 1.0810\n",
      "Epoch [60/100], Loss: 1.0611\n",
      "Epoch [70/100], Loss: 1.0531\n",
      "Epoch [80/100], Loss: 1.0496\n",
      "Epoch [90/100], Loss: 1.0479\n",
      "Epoch [100/100], Loss: 1.0469\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7375\n",
      "Epoch [20/100], Loss: 1.5542\n",
      "Epoch [30/100], Loss: 1.2806\n",
      "Epoch [40/100], Loss: 1.1358\n",
      "Epoch [50/100], Loss: 1.0843\n",
      "Epoch [60/100], Loss: 1.0628\n",
      "Epoch [70/100], Loss: 1.0541\n",
      "Epoch [80/100], Loss: 1.0503\n",
      "Epoch [90/100], Loss: 1.0484\n",
      "Epoch [100/100], Loss: 1.0473\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7379\n",
      "Epoch [20/100], Loss: 1.5583\n",
      "Epoch [30/100], Loss: 1.3038\n",
      "Epoch [40/100], Loss: 1.1464\n",
      "Epoch [50/100], Loss: 1.0890\n",
      "Epoch [60/100], Loss: 1.0654\n",
      "Epoch [70/100], Loss: 1.0554\n",
      "Epoch [80/100], Loss: 1.0508\n",
      "Epoch [90/100], Loss: 1.0486\n",
      "Epoch [100/100], Loss: 1.0474\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7353\n",
      "Epoch [20/100], Loss: 1.5449\n",
      "Epoch [30/100], Loss: 1.2741\n",
      "Epoch [40/100], Loss: 1.1293\n",
      "Epoch [50/100], Loss: 1.0807\n",
      "Epoch [60/100], Loss: 1.0610\n",
      "Epoch [70/100], Loss: 1.0529\n",
      "Epoch [80/100], Loss: 1.0494\n",
      "Epoch [90/100], Loss: 1.0478\n",
      "Epoch [100/100], Loss: 1.0468\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.7333\n",
      "Epoch [20/100], Loss: 1.5475\n",
      "Epoch [30/100], Loss: 1.2914\n",
      "Epoch [40/100], Loss: 1.1393\n",
      "Epoch [50/100], Loss: 1.0848\n",
      "Epoch [60/100], Loss: 1.0625\n",
      "Epoch [70/100], Loss: 1.0538\n",
      "Epoch [80/100], Loss: 1.0500\n",
      "Epoch [90/100], Loss: 1.0482\n",
      "Epoch [100/100], Loss: 1.0472\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7350\n",
      "Epoch [20/100], Loss: 1.5446\n",
      "Epoch [30/100], Loss: 1.2706\n",
      "Epoch [40/100], Loss: 1.1288\n",
      "Epoch [50/100], Loss: 1.0815\n",
      "Epoch [60/100], Loss: 1.0615\n",
      "Epoch [70/100], Loss: 1.0532\n",
      "Epoch [80/100], Loss: 1.0496\n",
      "Epoch [90/100], Loss: 1.0479\n",
      "Epoch [100/100], Loss: 1.0469\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.7403\n",
      "Epoch [20/100], Loss: 1.5569\n",
      "Epoch [30/100], Loss: 1.2794\n",
      "Epoch [40/100], Loss: 1.1333\n",
      "Epoch [50/100], Loss: 1.0825\n",
      "Epoch [60/100], Loss: 1.0617\n",
      "Epoch [70/100], Loss: 1.0534\n",
      "Epoch [80/100], Loss: 1.0498\n",
      "Epoch [90/100], Loss: 1.0480\n",
      "Epoch [100/100], Loss: 1.0471\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7298\n",
      "Epoch [20/100], Loss: 1.5310\n",
      "Epoch [30/100], Loss: 1.2701\n",
      "Epoch [40/100], Loss: 1.1327\n",
      "Epoch [50/100], Loss: 1.0847\n",
      "Epoch [60/100], Loss: 1.0634\n",
      "Epoch [70/100], Loss: 1.0545\n",
      "Epoch [80/100], Loss: 1.0505\n",
      "Epoch [90/100], Loss: 1.0485\n",
      "Epoch [100/100], Loss: 1.0474\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 1.7177\n",
      "Epoch [20/100], Loss: 1.5054\n",
      "Epoch [30/100], Loss: 1.2569\n",
      "Epoch [40/100], Loss: 1.1289\n",
      "Epoch [50/100], Loss: 1.0824\n",
      "Epoch [60/100], Loss: 1.0625\n",
      "Epoch [70/100], Loss: 1.0540\n",
      "Epoch [80/100], Loss: 1.0501\n",
      "Epoch [90/100], Loss: 1.0482\n",
      "Epoch [100/100], Loss: 1.0472\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7330\n",
      "Epoch [20/100], Loss: 1.5432\n",
      "Epoch [30/100], Loss: 1.2734\n",
      "Epoch [40/100], Loss: 1.1316\n",
      "Epoch [50/100], Loss: 1.0822\n",
      "Epoch [60/100], Loss: 1.0620\n",
      "Epoch [70/100], Loss: 1.0534\n",
      "Epoch [80/100], Loss: 1.0498\n",
      "Epoch [90/100], Loss: 1.0480\n",
      "Epoch [100/100], Loss: 1.0470\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.7289\n",
      "Epoch [20/100], Loss: 1.5309\n",
      "Epoch [30/100], Loss: 1.2679\n",
      "Epoch [40/100], Loss: 1.1282\n",
      "Epoch [50/100], Loss: 1.0807\n",
      "Epoch [60/100], Loss: 1.0615\n",
      "Epoch [70/100], Loss: 1.0535\n",
      "Epoch [80/100], Loss: 1.0499\n",
      "Epoch [90/100], Loss: 1.0481\n",
      "Epoch [100/100], Loss: 1.0471\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7338\n",
      "Epoch [20/100], Loss: 1.5380\n",
      "Epoch [30/100], Loss: 1.2615\n",
      "Epoch [40/100], Loss: 1.1220\n",
      "Epoch [50/100], Loss: 1.0755\n",
      "Epoch [60/100], Loss: 1.0582\n",
      "Epoch [70/100], Loss: 1.0515\n",
      "Epoch [80/100], Loss: 1.0487\n",
      "Epoch [90/100], Loss: 1.0473\n",
      "Epoch [100/100], Loss: 1.0465\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.7344\n",
      "Epoch [20/100], Loss: 1.5423\n",
      "Epoch [30/100], Loss: 1.2663\n",
      "Epoch [40/100], Loss: 1.1236\n",
      "Epoch [50/100], Loss: 1.0760\n",
      "Epoch [60/100], Loss: 1.0582\n",
      "Epoch [70/100], Loss: 1.0516\n",
      "Epoch [80/100], Loss: 1.0488\n",
      "Epoch [90/100], Loss: 1.0474\n",
      "Epoch [100/100], Loss: 1.0466\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7399\n",
      "Epoch [20/100], Loss: 1.5570\n",
      "Epoch [30/100], Loss: 1.2825\n",
      "Epoch [40/100], Loss: 1.1329\n",
      "Epoch [50/100], Loss: 1.0825\n",
      "Epoch [60/100], Loss: 1.0618\n",
      "Epoch [70/100], Loss: 1.0533\n",
      "Epoch [80/100], Loss: 1.0497\n",
      "Epoch [90/100], Loss: 1.0480\n",
      "Epoch [100/100], Loss: 1.0470\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.7267\n",
      "Epoch [20/100], Loss: 1.5222\n",
      "Epoch [30/100], Loss: 1.2565\n",
      "Epoch [40/100], Loss: 1.1235\n",
      "Epoch [50/100], Loss: 1.0783\n",
      "Epoch [60/100], Loss: 1.0604\n",
      "Epoch [70/100], Loss: 1.0526\n",
      "Epoch [80/100], Loss: 1.0493\n",
      "Epoch [90/100], Loss: 1.0477\n",
      "Epoch [100/100], Loss: 1.0468\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7351\n",
      "Epoch [20/100], Loss: 1.5432\n",
      "Epoch [30/100], Loss: 1.2732\n",
      "Epoch [40/100], Loss: 1.1315\n",
      "Epoch [50/100], Loss: 1.0827\n",
      "Epoch [60/100], Loss: 1.0623\n",
      "Epoch [70/100], Loss: 1.0537\n",
      "Epoch [80/100], Loss: 1.0500\n",
      "Epoch [90/100], Loss: 1.0482\n",
      "Epoch [100/100], Loss: 1.0471\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.7391\n",
      "Epoch [20/100], Loss: 1.5600\n",
      "Epoch [30/100], Loss: 1.2892\n",
      "Epoch [40/100], Loss: 1.1389\n",
      "Epoch [50/100], Loss: 1.0857\n",
      "Epoch [60/100], Loss: 1.0633\n",
      "Epoch [70/100], Loss: 1.0541\n",
      "Epoch [80/100], Loss: 1.0502\n",
      "Epoch [90/100], Loss: 1.0483\n",
      "Epoch [100/100], Loss: 1.0472\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7406\n",
      "Epoch [20/100], Loss: 1.5667\n",
      "Epoch [30/100], Loss: 1.2956\n",
      "Epoch [40/100], Loss: 1.1427\n",
      "Epoch [50/100], Loss: 1.0895\n",
      "Epoch [60/100], Loss: 1.0657\n",
      "Epoch [70/100], Loss: 1.0553\n",
      "Epoch [80/100], Loss: 1.0507\n",
      "Epoch [90/100], Loss: 1.0486\n",
      "Epoch [100/100], Loss: 1.0474\n",
      "Testing Time: 0.03 seconds\n",
      "Mean Training Time: 6.11 seconds\n",
      "Mean Test Accuracy: 0.6478\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.softmax(self.conv2(x, edge_index), dim=-1)\n",
    "        return x\n",
    "\n",
    "# Load the CiteSeer dataset\n",
    "dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 64, num_classes)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bZUq0Jgjlo4I"
   },
   "outputs": [],
   "source": [
    "#if the first layer activation be linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpAj_1eylu7a",
    "outputId": "196bf04d-240c-4c35-a8d3-47426aab74fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.8750\n",
      "Epoch [20/100], Loss: 1.6721\n",
      "Epoch [30/100], Loss: 1.4089\n",
      "Epoch [40/100], Loss: 1.2638\n",
      "Epoch [50/100], Loss: 1.2097\n",
      "Epoch [60/100], Loss: 1.1890\n",
      "Epoch [70/100], Loss: 1.1808\n",
      "Epoch [80/100], Loss: 1.1740\n",
      "Epoch [90/100], Loss: 1.1706\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8735\n",
      "Epoch [20/100], Loss: 1.6739\n",
      "Epoch [30/100], Loss: 1.4131\n",
      "Epoch [40/100], Loss: 1.2653\n",
      "Epoch [50/100], Loss: 1.2099\n",
      "Epoch [60/100], Loss: 1.1888\n",
      "Epoch [70/100], Loss: 1.1779\n",
      "Epoch [80/100], Loss: 1.1728\n",
      "Epoch [90/100], Loss: 1.1706\n",
      "Epoch [100/100], Loss: 1.1694\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8717\n",
      "Epoch [20/100], Loss: 1.6735\n",
      "Epoch [30/100], Loss: 1.4104\n",
      "Epoch [40/100], Loss: 1.2625\n",
      "Epoch [50/100], Loss: 1.2087\n",
      "Epoch [60/100], Loss: 1.1886\n",
      "Epoch [70/100], Loss: 1.1805\n",
      "Epoch [80/100], Loss: 1.1738\n",
      "Epoch [90/100], Loss: 1.1706\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8746\n",
      "Epoch [20/100], Loss: 1.6716\n",
      "Epoch [30/100], Loss: 1.4062\n",
      "Epoch [40/100], Loss: 1.2614\n",
      "Epoch [50/100], Loss: 1.2082\n",
      "Epoch [60/100], Loss: 1.1881\n",
      "Epoch [70/100], Loss: 1.1781\n",
      "Epoch [80/100], Loss: 1.1724\n",
      "Epoch [90/100], Loss: 1.1702\n",
      "Epoch [100/100], Loss: 1.1690\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8753\n",
      "Epoch [20/100], Loss: 1.6814\n",
      "Epoch [30/100], Loss: 1.4238\n",
      "Epoch [40/100], Loss: 1.2726\n",
      "Epoch [50/100], Loss: 1.2125\n",
      "Epoch [60/100], Loss: 1.1864\n",
      "Epoch [70/100], Loss: 1.1761\n",
      "Epoch [80/100], Loss: 1.1722\n",
      "Epoch [90/100], Loss: 1.1702\n",
      "Epoch [100/100], Loss: 1.1691\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8766\n",
      "Epoch [20/100], Loss: 1.6792\n",
      "Epoch [30/100], Loss: 1.4154\n",
      "Epoch [40/100], Loss: 1.2655\n",
      "Epoch [50/100], Loss: 1.2093\n",
      "Epoch [60/100], Loss: 1.1887\n",
      "Epoch [70/100], Loss: 1.1805\n",
      "Epoch [80/100], Loss: 1.1737\n",
      "Epoch [90/100], Loss: 1.1706\n",
      "Epoch [100/100], Loss: 1.1693\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8749\n",
      "Epoch [20/100], Loss: 1.6836\n",
      "Epoch [30/100], Loss: 1.4212\n",
      "Epoch [40/100], Loss: 1.2687\n",
      "Epoch [50/100], Loss: 1.2111\n",
      "Epoch [60/100], Loss: 1.1894\n",
      "Epoch [70/100], Loss: 1.1813\n",
      "Epoch [80/100], Loss: 1.1759\n",
      "Epoch [90/100], Loss: 1.1715\n",
      "Epoch [100/100], Loss: 1.1694\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8749\n",
      "Epoch [20/100], Loss: 1.6707\n",
      "Epoch [30/100], Loss: 1.4045\n",
      "Epoch [40/100], Loss: 1.2611\n",
      "Epoch [50/100], Loss: 1.2081\n",
      "Epoch [60/100], Loss: 1.1878\n",
      "Epoch [70/100], Loss: 1.1770\n",
      "Epoch [80/100], Loss: 1.1721\n",
      "Epoch [90/100], Loss: 1.1701\n",
      "Epoch [100/100], Loss: 1.1690\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8747\n",
      "Epoch [20/100], Loss: 1.6702\n",
      "Epoch [30/100], Loss: 1.4064\n",
      "Epoch [40/100], Loss: 1.2623\n",
      "Epoch [50/100], Loss: 1.2086\n",
      "Epoch [60/100], Loss: 1.1871\n",
      "Epoch [70/100], Loss: 1.1766\n",
      "Epoch [80/100], Loss: 1.1724\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/100], Loss: 1.8746\n",
      "Epoch [20/100], Loss: 1.6722\n",
      "Epoch [30/100], Loss: 1.4055\n",
      "Epoch [40/100], Loss: 1.2624\n",
      "Epoch [50/100], Loss: 1.2100\n",
      "Epoch [60/100], Loss: 1.1881\n",
      "Epoch [70/100], Loss: 1.1770\n",
      "Epoch [80/100], Loss: 1.1725\n",
      "Epoch [90/100], Loss: 1.1704\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8727\n",
      "Epoch [20/100], Loss: 1.6738\n",
      "Epoch [30/100], Loss: 1.4131\n",
      "Epoch [40/100], Loss: 1.2647\n",
      "Epoch [50/100], Loss: 1.2098\n",
      "Epoch [60/100], Loss: 1.1881\n",
      "Epoch [70/100], Loss: 1.1770\n",
      "Epoch [80/100], Loss: 1.1723\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8747\n",
      "Epoch [20/100], Loss: 1.6716\n",
      "Epoch [30/100], Loss: 1.4094\n",
      "Epoch [40/100], Loss: 1.2644\n",
      "Epoch [50/100], Loss: 1.2093\n",
      "Epoch [60/100], Loss: 1.1856\n",
      "Epoch [70/100], Loss: 1.1759\n",
      "Epoch [80/100], Loss: 1.1722\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8761\n",
      "Epoch [20/100], Loss: 1.6845\n",
      "Epoch [30/100], Loss: 1.4251\n",
      "Epoch [40/100], Loss: 1.2712\n",
      "Epoch [50/100], Loss: 1.2125\n",
      "Epoch [60/100], Loss: 1.1899\n",
      "Epoch [70/100], Loss: 1.1816\n",
      "Epoch [80/100], Loss: 1.1782\n",
      "Epoch [90/100], Loss: 1.1733\n",
      "Epoch [100/100], Loss: 1.1702\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8736\n",
      "Epoch [20/100], Loss: 1.6749\n",
      "Epoch [30/100], Loss: 1.4127\n",
      "Epoch [40/100], Loss: 1.2642\n",
      "Epoch [50/100], Loss: 1.2096\n",
      "Epoch [60/100], Loss: 1.1888\n",
      "Epoch [70/100], Loss: 1.1781\n",
      "Epoch [80/100], Loss: 1.1726\n",
      "Epoch [90/100], Loss: 1.1705\n",
      "Epoch [100/100], Loss: 1.1693\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8770\n",
      "Epoch [20/100], Loss: 1.6775\n",
      "Epoch [30/100], Loss: 1.4130\n",
      "Epoch [40/100], Loss: 1.2648\n",
      "Epoch [50/100], Loss: 1.2096\n",
      "Epoch [60/100], Loss: 1.1869\n",
      "Epoch [70/100], Loss: 1.1765\n",
      "Epoch [80/100], Loss: 1.1724\n",
      "Epoch [90/100], Loss: 1.1704\n",
      "Epoch [100/100], Loss: 1.1693\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8747\n",
      "Epoch [20/100], Loss: 1.6717\n",
      "Epoch [30/100], Loss: 1.4102\n",
      "Epoch [40/100], Loss: 1.2656\n",
      "Epoch [50/100], Loss: 1.2099\n",
      "Epoch [60/100], Loss: 1.1857\n",
      "Epoch [70/100], Loss: 1.1756\n",
      "Epoch [80/100], Loss: 1.1719\n",
      "Epoch [90/100], Loss: 1.1700\n",
      "Epoch [100/100], Loss: 1.1689\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8734\n",
      "Epoch [20/100], Loss: 1.6771\n",
      "Epoch [30/100], Loss: 1.4164\n",
      "Epoch [40/100], Loss: 1.2685\n",
      "Epoch [50/100], Loss: 1.2118\n",
      "Epoch [60/100], Loss: 1.1873\n",
      "Epoch [70/100], Loss: 1.1764\n",
      "Epoch [80/100], Loss: 1.1723\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1691\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8741\n",
      "Epoch [20/100], Loss: 1.6764\n",
      "Epoch [30/100], Loss: 1.4184\n",
      "Epoch [40/100], Loss: 1.2676\n",
      "Epoch [50/100], Loss: 1.2105\n",
      "Epoch [60/100], Loss: 1.1887\n",
      "Epoch [70/100], Loss: 1.1776\n",
      "Epoch [80/100], Loss: 1.1724\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/100], Loss: 1.8753\n",
      "Epoch [20/100], Loss: 1.6778\n",
      "Epoch [30/100], Loss: 1.4107\n",
      "Epoch [40/100], Loss: 1.2627\n",
      "Epoch [50/100], Loss: 1.2085\n",
      "Epoch [60/100], Loss: 1.1863\n",
      "Epoch [70/100], Loss: 1.1762\n",
      "Epoch [80/100], Loss: 1.1722\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/100], Loss: 1.8732\n",
      "Epoch [20/100], Loss: 1.6699\n",
      "Epoch [30/100], Loss: 1.4068\n",
      "Epoch [40/100], Loss: 1.2624\n",
      "Epoch [50/100], Loss: 1.2090\n",
      "Epoch [60/100], Loss: 1.1886\n",
      "Epoch [70/100], Loss: 1.1780\n",
      "Epoch [80/100], Loss: 1.1726\n",
      "Epoch [90/100], Loss: 1.1703\n",
      "Epoch [100/100], Loss: 1.1692\n",
      "Testing Time: 0.01 seconds\n",
      "Mean Training Time: 2.52 seconds\n",
      "Mean Test Accuracy: 0.7951\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = (self.conv1(x, edge_index))\n",
    "        x = F.softmax(self.conv2(x, edge_index), dim=-1)\n",
    "        return x\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 64, num_classes)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZuD3fcCFmAD4",
    "outputId": "3c454846-2a3f-4f2a-9a6f-1fb837819872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.9856\n",
      "Epoch [20/100], Loss: 0.8088\n",
      "Epoch [30/100], Loss: 0.6749\n",
      "Epoch [40/100], Loss: 0.6110\n",
      "Epoch [50/100], Loss: 0.5856\n",
      "Epoch [60/100], Loss: 0.5738\n",
      "Epoch [70/100], Loss: 0.5627\n",
      "Epoch [80/100], Loss: 0.5584\n",
      "Epoch [90/100], Loss: 0.5560\n",
      "Epoch [100/100], Loss: 0.5549\n",
      "Testing Time: 0.06 seconds\n",
      "Epoch [10/100], Loss: 0.9965\n",
      "Epoch [20/100], Loss: 0.8254\n",
      "Epoch [30/100], Loss: 0.6911\n",
      "Epoch [40/100], Loss: 0.6197\n",
      "Epoch [50/100], Loss: 0.5879\n",
      "Epoch [60/100], Loss: 0.5691\n",
      "Epoch [70/100], Loss: 0.5615\n",
      "Epoch [80/100], Loss: 0.5577\n",
      "Epoch [90/100], Loss: 0.5560\n",
      "Epoch [100/100], Loss: 0.5550\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 0.9876\n",
      "Epoch [20/100], Loss: 0.8022\n",
      "Epoch [30/100], Loss: 0.6669\n",
      "Epoch [40/100], Loss: 0.6070\n",
      "Epoch [50/100], Loss: 0.5840\n",
      "Epoch [60/100], Loss: 0.5749\n",
      "Epoch [70/100], Loss: 0.5641\n",
      "Epoch [80/100], Loss: 0.5589\n",
      "Epoch [90/100], Loss: 0.5563\n",
      "Epoch [100/100], Loss: 0.5550\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9866\n",
      "Epoch [20/100], Loss: 0.8054\n",
      "Epoch [30/100], Loss: 0.6714\n",
      "Epoch [40/100], Loss: 0.6092\n",
      "Epoch [50/100], Loss: 0.5848\n",
      "Epoch [60/100], Loss: 0.5734\n",
      "Epoch [70/100], Loss: 0.5628\n",
      "Epoch [80/100], Loss: 0.5584\n",
      "Epoch [90/100], Loss: 0.5560\n",
      "Epoch [100/100], Loss: 0.5549\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 0.9823\n",
      "Epoch [20/100], Loss: 0.7945\n",
      "Epoch [30/100], Loss: 0.6628\n",
      "Epoch [40/100], Loss: 0.6055\n",
      "Epoch [50/100], Loss: 0.5833\n",
      "Epoch [60/100], Loss: 0.5712\n",
      "Epoch [70/100], Loss: 0.5618\n",
      "Epoch [80/100], Loss: 0.5579\n",
      "Epoch [90/100], Loss: 0.5558\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9823\n",
      "Epoch [20/100], Loss: 0.7955\n",
      "Epoch [30/100], Loss: 0.6621\n",
      "Epoch [40/100], Loss: 0.6046\n",
      "Epoch [50/100], Loss: 0.5828\n",
      "Epoch [60/100], Loss: 0.5718\n",
      "Epoch [70/100], Loss: 0.5622\n",
      "Epoch [80/100], Loss: 0.5581\n",
      "Epoch [90/100], Loss: 0.5559\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9854\n",
      "Epoch [20/100], Loss: 0.7994\n",
      "Epoch [30/100], Loss: 0.6655\n",
      "Epoch [40/100], Loss: 0.6066\n",
      "Epoch [50/100], Loss: 0.5841\n",
      "Epoch [60/100], Loss: 0.5756\n",
      "Epoch [70/100], Loss: 0.5656\n",
      "Epoch [80/100], Loss: 0.5591\n",
      "Epoch [90/100], Loss: 0.5564\n",
      "Epoch [100/100], Loss: 0.5550\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9948\n",
      "Epoch [20/100], Loss: 0.8147\n",
      "Epoch [30/100], Loss: 0.6758\n",
      "Epoch [40/100], Loss: 0.6107\n",
      "Epoch [50/100], Loss: 0.5823\n",
      "Epoch [60/100], Loss: 0.5671\n",
      "Epoch [70/100], Loss: 0.5604\n",
      "Epoch [80/100], Loss: 0.5571\n",
      "Epoch [90/100], Loss: 0.5556\n",
      "Epoch [100/100], Loss: 0.5547\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9787\n",
      "Epoch [20/100], Loss: 0.8050\n",
      "Epoch [30/100], Loss: 0.6755\n",
      "Epoch [40/100], Loss: 0.6117\n",
      "Epoch [50/100], Loss: 0.5864\n",
      "Epoch [60/100], Loss: 0.5774\n",
      "Epoch [70/100], Loss: 0.5738\n",
      "Epoch [80/100], Loss: 0.5722\n",
      "Epoch [90/100], Loss: 0.5712\n",
      "Epoch [100/100], Loss: 0.5707\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9829\n",
      "Epoch [20/100], Loss: 0.7991\n",
      "Epoch [30/100], Loss: 0.6674\n",
      "Epoch [40/100], Loss: 0.6078\n",
      "Epoch [50/100], Loss: 0.5844\n",
      "Epoch [60/100], Loss: 0.5729\n",
      "Epoch [70/100], Loss: 0.5624\n",
      "Epoch [80/100], Loss: 0.5582\n",
      "Epoch [90/100], Loss: 0.5559\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9873\n",
      "Epoch [20/100], Loss: 0.8084\n",
      "Epoch [30/100], Loss: 0.6745\n",
      "Epoch [40/100], Loss: 0.6111\n",
      "Epoch [50/100], Loss: 0.5851\n",
      "Epoch [60/100], Loss: 0.5690\n",
      "Epoch [70/100], Loss: 0.5614\n",
      "Epoch [80/100], Loss: 0.5575\n",
      "Epoch [90/100], Loss: 0.5558\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 0.9926\n",
      "Epoch [20/100], Loss: 0.8074\n",
      "Epoch [30/100], Loss: 0.6700\n",
      "Epoch [40/100], Loss: 0.6085\n",
      "Epoch [50/100], Loss: 0.5842\n",
      "Epoch [60/100], Loss: 0.5698\n",
      "Epoch [70/100], Loss: 0.5615\n",
      "Epoch [80/100], Loss: 0.5576\n",
      "Epoch [90/100], Loss: 0.5557\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.06 seconds\n",
      "Epoch [10/100], Loss: 0.9818\n",
      "Epoch [20/100], Loss: 0.7954\n",
      "Epoch [30/100], Loss: 0.6629\n",
      "Epoch [40/100], Loss: 0.6054\n",
      "Epoch [50/100], Loss: 0.5837\n",
      "Epoch [60/100], Loss: 0.5752\n",
      "Epoch [70/100], Loss: 0.5648\n",
      "Epoch [80/100], Loss: 0.5589\n",
      "Epoch [90/100], Loss: 0.5563\n",
      "Epoch [100/100], Loss: 0.5549\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 0.9859\n",
      "Epoch [20/100], Loss: 0.8074\n",
      "Epoch [30/100], Loss: 0.6738\n",
      "Epoch [40/100], Loss: 0.6106\n",
      "Epoch [50/100], Loss: 0.5851\n",
      "Epoch [60/100], Loss: 0.5698\n",
      "Epoch [70/100], Loss: 0.5616\n",
      "Epoch [80/100], Loss: 0.5576\n",
      "Epoch [90/100], Loss: 0.5557\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 0.9805\n",
      "Epoch [20/100], Loss: 0.7932\n",
      "Epoch [30/100], Loss: 0.6612\n",
      "Epoch [40/100], Loss: 0.6046\n",
      "Epoch [50/100], Loss: 0.5834\n",
      "Epoch [60/100], Loss: 0.5756\n",
      "Epoch [70/100], Loss: 0.5678\n",
      "Epoch [80/100], Loss: 0.5597\n",
      "Epoch [90/100], Loss: 0.5567\n",
      "Epoch [100/100], Loss: 0.5551\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9856\n",
      "Epoch [20/100], Loss: 0.8085\n",
      "Epoch [30/100], Loss: 0.6729\n",
      "Epoch [40/100], Loss: 0.6090\n",
      "Epoch [50/100], Loss: 0.5793\n",
      "Epoch [60/100], Loss: 0.5660\n",
      "Epoch [70/100], Loss: 0.5599\n",
      "Epoch [80/100], Loss: 0.5571\n",
      "Epoch [90/100], Loss: 0.5556\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/100], Loss: 0.9834\n",
      "Epoch [20/100], Loss: 0.8002\n",
      "Epoch [30/100], Loss: 0.6678\n",
      "Epoch [40/100], Loss: 0.6079\n",
      "Epoch [50/100], Loss: 0.5845\n",
      "Epoch [60/100], Loss: 0.5728\n",
      "Epoch [70/100], Loss: 0.5624\n",
      "Epoch [80/100], Loss: 0.5581\n",
      "Epoch [90/100], Loss: 0.5559\n",
      "Epoch [100/100], Loss: 0.5548\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9854\n",
      "Epoch [20/100], Loss: 0.8090\n",
      "Epoch [30/100], Loss: 0.6755\n",
      "Epoch [40/100], Loss: 0.6111\n",
      "Epoch [50/100], Loss: 0.5858\n",
      "Epoch [60/100], Loss: 0.5748\n",
      "Epoch [70/100], Loss: 0.5631\n",
      "Epoch [80/100], Loss: 0.5585\n",
      "Epoch [90/100], Loss: 0.5560\n",
      "Epoch [100/100], Loss: 0.5549\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9908\n",
      "Epoch [20/100], Loss: 0.8070\n",
      "Epoch [30/100], Loss: 0.6709\n",
      "Epoch [40/100], Loss: 0.6092\n",
      "Epoch [50/100], Loss: 0.5852\n",
      "Epoch [60/100], Loss: 0.5762\n",
      "Epoch [70/100], Loss: 0.5664\n",
      "Epoch [80/100], Loss: 0.5594\n",
      "Epoch [90/100], Loss: 0.5566\n",
      "Epoch [100/100], Loss: 0.5551\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 0.9868\n",
      "Epoch [20/100], Loss: 0.8011\n",
      "Epoch [30/100], Loss: 0.6648\n",
      "Epoch [40/100], Loss: 0.6057\n",
      "Epoch [50/100], Loss: 0.5833\n",
      "Epoch [60/100], Loss: 0.5740\n",
      "Epoch [70/100], Loss: 0.5636\n",
      "Epoch [80/100], Loss: 0.5587\n",
      "Epoch [90/100], Loss: 0.5562\n",
      "Epoch [100/100], Loss: 0.5549\n",
      "Testing Time: 0.04 seconds\n",
      "Mean Training Time: 8.77 seconds\n",
      "Mean Test Accuracy: 0.7592\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = (self.conv1(x, edge_index))\n",
    "        x = F.softmax(self.conv2(x, edge_index), dim=-1)\n",
    "        return x\n",
    "\n",
    "# Load the PubMed dataset\n",
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 64, num_classes)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b67wcdtWqj_x",
    "outputId": "84269949-7bcf-47ca-98f4-779ed9aa0fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.7037\n",
      "Epoch [20/100], Loss: 1.4648\n",
      "Epoch [30/100], Loss: 1.2169\n",
      "Epoch [40/100], Loss: 1.1132\n",
      "Epoch [50/100], Loss: 1.0752\n",
      "Epoch [60/100], Loss: 1.0593\n",
      "Epoch [70/100], Loss: 1.0523\n",
      "Epoch [80/100], Loss: 1.0493\n",
      "Epoch [90/100], Loss: 1.0477\n",
      "Epoch [100/100], Loss: 1.0468\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.6989\n",
      "Epoch [20/100], Loss: 1.4705\n",
      "Epoch [30/100], Loss: 1.2271\n",
      "Epoch [40/100], Loss: 1.1143\n",
      "Epoch [50/100], Loss: 1.0746\n",
      "Epoch [60/100], Loss: 1.0586\n",
      "Epoch [70/100], Loss: 1.0518\n",
      "Epoch [80/100], Loss: 1.0489\n",
      "Epoch [90/100], Loss: 1.0474\n",
      "Epoch [100/100], Loss: 1.0466\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7032\n",
      "Epoch [20/100], Loss: 1.4603\n",
      "Epoch [30/100], Loss: 1.2091\n",
      "Epoch [40/100], Loss: 1.1098\n",
      "Epoch [50/100], Loss: 1.0738\n",
      "Epoch [60/100], Loss: 1.0578\n",
      "Epoch [70/100], Loss: 1.0513\n",
      "Epoch [80/100], Loss: 1.0486\n",
      "Epoch [90/100], Loss: 1.0472\n",
      "Epoch [100/100], Loss: 1.0465\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.6995\n",
      "Epoch [20/100], Loss: 1.4521\n",
      "Epoch [30/100], Loss: 1.2042\n",
      "Epoch [40/100], Loss: 1.1078\n",
      "Epoch [50/100], Loss: 1.0729\n",
      "Epoch [60/100], Loss: 1.0576\n",
      "Epoch [70/100], Loss: 1.0514\n",
      "Epoch [80/100], Loss: 1.0487\n",
      "Epoch [90/100], Loss: 1.0473\n",
      "Epoch [100/100], Loss: 1.0466\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.6993\n",
      "Epoch [20/100], Loss: 1.4587\n",
      "Epoch [30/100], Loss: 1.2123\n",
      "Epoch [40/100], Loss: 1.1089\n",
      "Epoch [50/100], Loss: 1.0721\n",
      "Epoch [60/100], Loss: 1.0575\n",
      "Epoch [70/100], Loss: 1.0515\n",
      "Epoch [80/100], Loss: 1.0488\n",
      "Epoch [90/100], Loss: 1.0474\n",
      "Epoch [100/100], Loss: 1.0466\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7024\n",
      "Epoch [20/100], Loss: 1.4611\n",
      "Epoch [30/100], Loss: 1.2114\n",
      "Epoch [40/100], Loss: 1.1100\n",
      "Epoch [50/100], Loss: 1.0732\n",
      "Epoch [60/100], Loss: 1.0579\n",
      "Epoch [70/100], Loss: 1.0516\n",
      "Epoch [80/100], Loss: 1.0489\n",
      "Epoch [90/100], Loss: 1.0475\n",
      "Epoch [100/100], Loss: 1.0467\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.6941\n",
      "Epoch [20/100], Loss: 1.4570\n",
      "Epoch [30/100], Loss: 1.2218\n",
      "Epoch [40/100], Loss: 1.1159\n",
      "Epoch [50/100], Loss: 1.0763\n",
      "Epoch [60/100], Loss: 1.0595\n",
      "Epoch [70/100], Loss: 1.0525\n",
      "Epoch [80/100], Loss: 1.0493\n",
      "Epoch [90/100], Loss: 1.0477\n",
      "Epoch [100/100], Loss: 1.0468\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7007\n",
      "Epoch [20/100], Loss: 1.4639\n",
      "Epoch [30/100], Loss: 1.2138\n",
      "Epoch [40/100], Loss: 1.1095\n",
      "Epoch [50/100], Loss: 1.0727\n",
      "Epoch [60/100], Loss: 1.0575\n",
      "Epoch [70/100], Loss: 1.0513\n",
      "Epoch [80/100], Loss: 1.0486\n",
      "Epoch [90/100], Loss: 1.0473\n",
      "Epoch [100/100], Loss: 1.0465\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7000\n",
      "Epoch [20/100], Loss: 1.4630\n",
      "Epoch [30/100], Loss: 1.2134\n",
      "Epoch [40/100], Loss: 1.1094\n",
      "Epoch [50/100], Loss: 1.0722\n",
      "Epoch [60/100], Loss: 1.0572\n",
      "Epoch [70/100], Loss: 1.0512\n",
      "Epoch [80/100], Loss: 1.0486\n",
      "Epoch [90/100], Loss: 1.0472\n",
      "Epoch [100/100], Loss: 1.0465\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7001\n",
      "Epoch [20/100], Loss: 1.4612\n",
      "Epoch [30/100], Loss: 1.2182\n",
      "Epoch [40/100], Loss: 1.1139\n",
      "Epoch [50/100], Loss: 1.0760\n",
      "Epoch [60/100], Loss: 1.0594\n",
      "Epoch [70/100], Loss: 1.0523\n",
      "Epoch [80/100], Loss: 1.0492\n",
      "Epoch [90/100], Loss: 1.0476\n",
      "Epoch [100/100], Loss: 1.0467\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.6978\n",
      "Epoch [20/100], Loss: 1.4527\n",
      "Epoch [30/100], Loss: 1.2045\n",
      "Epoch [40/100], Loss: 1.1063\n",
      "Epoch [50/100], Loss: 1.0717\n",
      "Epoch [60/100], Loss: 1.0574\n",
      "Epoch [70/100], Loss: 1.0514\n",
      "Epoch [80/100], Loss: 1.0488\n",
      "Epoch [90/100], Loss: 1.0474\n",
      "Epoch [100/100], Loss: 1.0466\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7011\n",
      "Epoch [20/100], Loss: 1.4605\n",
      "Epoch [30/100], Loss: 1.2106\n",
      "Epoch [40/100], Loss: 1.1080\n",
      "Epoch [50/100], Loss: 1.0718\n",
      "Epoch [60/100], Loss: 1.0573\n",
      "Epoch [70/100], Loss: 1.0514\n",
      "Epoch [80/100], Loss: 1.0488\n",
      "Epoch [90/100], Loss: 1.0474\n",
      "Epoch [100/100], Loss: 1.0466\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.6986\n",
      "Epoch [20/100], Loss: 1.4529\n",
      "Epoch [30/100], Loss: 1.2074\n",
      "Epoch [40/100], Loss: 1.1074\n",
      "Epoch [50/100], Loss: 1.0719\n",
      "Epoch [60/100], Loss: 1.0575\n",
      "Epoch [70/100], Loss: 1.0516\n",
      "Epoch [80/100], Loss: 1.0489\n",
      "Epoch [90/100], Loss: 1.0475\n",
      "Epoch [100/100], Loss: 1.0467\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7017\n",
      "Epoch [20/100], Loss: 1.4610\n",
      "Epoch [30/100], Loss: 1.2142\n",
      "Epoch [40/100], Loss: 1.1115\n",
      "Epoch [50/100], Loss: 1.0741\n",
      "Epoch [60/100], Loss: 1.0585\n",
      "Epoch [70/100], Loss: 1.0520\n",
      "Epoch [80/100], Loss: 1.0491\n",
      "Epoch [90/100], Loss: 1.0476\n",
      "Epoch [100/100], Loss: 1.0468\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.6964\n",
      "Epoch [20/100], Loss: 1.4584\n",
      "Epoch [30/100], Loss: 1.2135\n",
      "Epoch [40/100], Loss: 1.1094\n",
      "Epoch [50/100], Loss: 1.0727\n",
      "Epoch [60/100], Loss: 1.0579\n",
      "Epoch [70/100], Loss: 1.0516\n",
      "Epoch [80/100], Loss: 1.0488\n",
      "Epoch [90/100], Loss: 1.0474\n",
      "Epoch [100/100], Loss: 1.0466\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7013\n",
      "Epoch [20/100], Loss: 1.4663\n",
      "Epoch [30/100], Loss: 1.2223\n",
      "Epoch [40/100], Loss: 1.1146\n",
      "Epoch [50/100], Loss: 1.0746\n",
      "Epoch [60/100], Loss: 1.0587\n",
      "Epoch [70/100], Loss: 1.0521\n",
      "Epoch [80/100], Loss: 1.0491\n",
      "Epoch [90/100], Loss: 1.0476\n",
      "Epoch [100/100], Loss: 1.0468\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.7025\n",
      "Epoch [20/100], Loss: 1.4663\n",
      "Epoch [30/100], Loss: 1.2189\n",
      "Epoch [40/100], Loss: 1.1127\n",
      "Epoch [50/100], Loss: 1.0742\n",
      "Epoch [60/100], Loss: 1.0583\n",
      "Epoch [70/100], Loss: 1.0518\n",
      "Epoch [80/100], Loss: 1.0489\n",
      "Epoch [90/100], Loss: 1.0475\n",
      "Epoch [100/100], Loss: 1.0466\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7037\n",
      "Epoch [20/100], Loss: 1.4633\n",
      "Epoch [30/100], Loss: 1.2138\n",
      "Epoch [40/100], Loss: 1.1114\n",
      "Epoch [50/100], Loss: 1.0741\n",
      "Epoch [60/100], Loss: 1.0584\n",
      "Epoch [70/100], Loss: 1.0519\n",
      "Epoch [80/100], Loss: 1.0491\n",
      "Epoch [90/100], Loss: 1.0476\n",
      "Epoch [100/100], Loss: 1.0467\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/100], Loss: 1.7002\n",
      "Epoch [20/100], Loss: 1.4598\n",
      "Epoch [30/100], Loss: 1.2115\n",
      "Epoch [40/100], Loss: 1.1087\n",
      "Epoch [50/100], Loss: 1.0723\n",
      "Epoch [60/100], Loss: 1.0574\n",
      "Epoch [70/100], Loss: 1.0513\n",
      "Epoch [80/100], Loss: 1.0486\n",
      "Epoch [90/100], Loss: 1.0473\n",
      "Epoch [100/100], Loss: 1.0465\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/100], Loss: 1.7017\n",
      "Epoch [20/100], Loss: 1.4672\n",
      "Epoch [30/100], Loss: 1.2150\n",
      "Epoch [40/100], Loss: 1.1087\n",
      "Epoch [50/100], Loss: 1.0719\n",
      "Epoch [60/100], Loss: 1.0571\n",
      "Epoch [70/100], Loss: 1.0511\n",
      "Epoch [80/100], Loss: 1.0485\n",
      "Epoch [90/100], Loss: 1.0472\n",
      "Epoch [100/100], Loss: 1.0465\n",
      "Testing Time: 0.03 seconds\n",
      "Mean Training Time: 6.04 seconds\n",
      "Mean Test Accuracy: 0.6443\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = (self.conv1(x, edge_index))\n",
    "        x = F.softmax(self.conv2(x, edge_index), dim=-1)\n",
    "        return x\n",
    "\n",
    "# Load the CiteSeer dataset\n",
    "dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 64, num_classes)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhroS5kXvtzA",
    "outputId": "6ad13c6e-d6b4-49fd-bb33-1f8e9ec20547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 1.7988\n",
      "Epoch [20/200], Loss: 1.6264\n",
      "Epoch [30/200], Loss: 1.3328\n",
      "Epoch [40/200], Loss: 1.1695\n",
      "Epoch [50/200], Loss: 0.9769\n",
      "Epoch [60/200], Loss: 1.0086\n",
      "Epoch [70/200], Loss: 0.7957\n",
      "Epoch [80/200], Loss: 0.8097\n",
      "Early stopping at epoch 89\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8261\n",
      "Epoch [20/200], Loss: 1.6007\n",
      "Epoch [30/200], Loss: 1.3883\n",
      "Epoch [40/200], Loss: 1.0876\n",
      "Epoch [50/200], Loss: 0.9260\n",
      "Epoch [60/200], Loss: 0.9679\n",
      "Early stopping at epoch 68\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8461\n",
      "Epoch [20/200], Loss: 1.6083\n",
      "Epoch [30/200], Loss: 1.3992\n",
      "Epoch [40/200], Loss: 1.2669\n",
      "Epoch [50/200], Loss: 1.0697\n",
      "Early stopping at epoch 58\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8482\n",
      "Epoch [20/200], Loss: 1.7004\n",
      "Epoch [30/200], Loss: 1.4473\n",
      "Epoch [40/200], Loss: 1.1913\n",
      "Epoch [50/200], Loss: 1.0063\n",
      "Epoch [60/200], Loss: 0.9295\n",
      "Epoch [70/200], Loss: 0.8447\n",
      "Epoch [80/200], Loss: 0.7695\n",
      "Epoch [90/200], Loss: 0.8394\n",
      "Early stopping at epoch 99\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8574\n",
      "Epoch [20/200], Loss: 1.6566\n",
      "Epoch [30/200], Loss: 1.3516\n",
      "Epoch [40/200], Loss: 1.2272\n",
      "Epoch [50/200], Loss: 0.9920\n",
      "Epoch [60/200], Loss: 1.0032\n",
      "Epoch [70/200], Loss: 0.9196\n",
      "Epoch [80/200], Loss: 0.7657\n",
      "Epoch [90/200], Loss: 0.7708\n",
      "Early stopping at epoch 90\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8272\n",
      "Epoch [20/200], Loss: 1.6916\n",
      "Epoch [30/200], Loss: 1.4482\n",
      "Epoch [40/200], Loss: 1.1950\n",
      "Epoch [50/200], Loss: 1.0858\n",
      "Epoch [60/200], Loss: 0.9887\n",
      "Epoch [70/200], Loss: 0.9790\n",
      "Early stopping at epoch 76\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8426\n",
      "Epoch [20/200], Loss: 1.6649\n",
      "Epoch [30/200], Loss: 1.4556\n",
      "Epoch [40/200], Loss: 1.1960\n",
      "Epoch [50/200], Loss: 1.0143\n",
      "Epoch [60/200], Loss: 0.9513\n",
      "Epoch [70/200], Loss: 0.8542\n",
      "Epoch [80/200], Loss: 0.8793\n",
      "Epoch [90/200], Loss: 0.9065\n",
      "Early stopping at epoch 92\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8286\n",
      "Epoch [20/200], Loss: 1.7115\n",
      "Epoch [30/200], Loss: 1.4160\n",
      "Epoch [40/200], Loss: 1.1030\n",
      "Epoch [50/200], Loss: 1.0004\n",
      "Epoch [60/200], Loss: 0.8369\n",
      "Epoch [70/200], Loss: 0.9012\n",
      "Epoch [80/200], Loss: 0.7957\n",
      "Early stopping at epoch 89\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8284\n",
      "Epoch [20/200], Loss: 1.6806\n",
      "Epoch [30/200], Loss: 1.3401\n",
      "Epoch [40/200], Loss: 1.2036\n",
      "Epoch [50/200], Loss: 0.9356\n",
      "Early stopping at epoch 56\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8222\n",
      "Epoch [20/200], Loss: 1.6263\n",
      "Epoch [30/200], Loss: 1.2826\n",
      "Epoch [40/200], Loss: 1.1283\n",
      "Epoch [50/200], Loss: 1.0286\n",
      "Epoch [60/200], Loss: 0.9468\n",
      "Epoch [70/200], Loss: 0.8860\n",
      "Epoch [80/200], Loss: 0.7817\n",
      "Early stopping at epoch 84\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8539\n",
      "Epoch [20/200], Loss: 1.6549\n",
      "Epoch [30/200], Loss: 1.4331\n",
      "Epoch [40/200], Loss: 1.1627\n",
      "Epoch [50/200], Loss: 1.0030\n",
      "Epoch [60/200], Loss: 0.9956\n",
      "Epoch [70/200], Loss: 0.8827\n",
      "Early stopping at epoch 75\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8578\n",
      "Epoch [20/200], Loss: 1.6013\n",
      "Epoch [30/200], Loss: 1.4296\n",
      "Epoch [40/200], Loss: 1.1198\n",
      "Epoch [50/200], Loss: 1.0244\n",
      "Epoch [60/200], Loss: 0.9796\n",
      "Epoch [70/200], Loss: 0.8686\n",
      "Epoch [80/200], Loss: 0.8160\n",
      "Epoch [90/200], Loss: 0.8646\n",
      "Epoch [100/200], Loss: 0.7587\n",
      "Epoch [110/200], Loss: 0.6856\n",
      "Early stopping at epoch 116\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8556\n",
      "Epoch [20/200], Loss: 1.6138\n",
      "Epoch [30/200], Loss: 1.3275\n",
      "Epoch [40/200], Loss: 1.1272\n",
      "Epoch [50/200], Loss: 1.0316\n",
      "Epoch [60/200], Loss: 0.9474\n",
      "Early stopping at epoch 69\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8303\n",
      "Epoch [20/200], Loss: 1.6376\n",
      "Epoch [30/200], Loss: 1.3845\n",
      "Epoch [40/200], Loss: 1.1221\n",
      "Epoch [50/200], Loss: 0.9477\n",
      "Epoch [60/200], Loss: 1.0205\n",
      "Early stopping at epoch 63\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8499\n",
      "Epoch [20/200], Loss: 1.6633\n",
      "Epoch [30/200], Loss: 1.4384\n",
      "Epoch [40/200], Loss: 1.2201\n",
      "Epoch [50/200], Loss: 1.0310\n",
      "Epoch [60/200], Loss: 0.9077\n",
      "Epoch [70/200], Loss: 0.9566\n",
      "Epoch [80/200], Loss: 0.8464\n",
      "Early stopping at epoch 86\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8162\n",
      "Epoch [20/200], Loss: 1.6128\n",
      "Epoch [30/200], Loss: 1.4096\n",
      "Epoch [40/200], Loss: 1.1636\n",
      "Epoch [50/200], Loss: 1.0200\n",
      "Epoch [60/200], Loss: 0.8916\n",
      "Epoch [70/200], Loss: 0.7698\n",
      "Epoch [80/200], Loss: 0.7489\n",
      "Early stopping at epoch 84\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8315\n",
      "Epoch [20/200], Loss: 1.5865\n",
      "Epoch [30/200], Loss: 1.3612\n",
      "Epoch [40/200], Loss: 1.2729\n",
      "Epoch [50/200], Loss: 0.8894\n",
      "Epoch [60/200], Loss: 0.8541\n",
      "Epoch [70/200], Loss: 0.7973\n",
      "Epoch [80/200], Loss: 0.8831\n",
      "Epoch [90/200], Loss: 0.8670\n",
      "Early stopping at epoch 94\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8326\n",
      "Epoch [20/200], Loss: 1.6394\n",
      "Epoch [30/200], Loss: 1.3230\n",
      "Epoch [40/200], Loss: 1.0791\n",
      "Epoch [50/200], Loss: 0.9907\n",
      "Early stopping at epoch 51\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8478\n",
      "Epoch [20/200], Loss: 1.6551\n",
      "Epoch [30/200], Loss: 1.3186\n",
      "Epoch [40/200], Loss: 1.1807\n",
      "Epoch [50/200], Loss: 0.9393\n",
      "Epoch [60/200], Loss: 0.9364\n",
      "Epoch [70/200], Loss: 0.8518\n",
      "Epoch [80/200], Loss: 0.8697\n",
      "Epoch [90/200], Loss: 0.8328\n",
      "Epoch [100/200], Loss: 0.7639\n",
      "Early stopping at epoch 101\n",
      "Testing Time: 0.01 seconds\n",
      "Epoch [10/200], Loss: 1.8569\n",
      "Epoch [20/200], Loss: 1.6640\n",
      "Epoch [30/200], Loss: 1.3532\n",
      "Epoch [40/200], Loss: 1.1727\n",
      "Epoch [50/200], Loss: 1.0592\n",
      "Epoch [60/200], Loss: 0.8691\n",
      "Epoch [70/200], Loss: 0.8148\n",
      "Early stopping at epoch 71\n",
      "Testing Time: 0.01 seconds\n",
      "Mean Training Time: 1.35 seconds\n",
      "Mean Test Accuracy: 0.8029\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)  # Apply dropout\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)  # Apply dropout\n",
    "        return x\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "window_size = 10\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 32, num_classes)  # Hidden layer size: 32 units\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= window_size:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xOCoLrXzmvz",
    "outputId": "a11c1f1b-be31-49f4-9ac0-7e38c19a93e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 1.0035\n",
      "Epoch [20/200], Loss: 0.8738\n",
      "Epoch [30/200], Loss: 0.7093\n",
      "Epoch [40/200], Loss: 0.6109\n",
      "Epoch [50/200], Loss: 0.5639\n",
      "Epoch [60/200], Loss: 0.4661\n",
      "Epoch [70/200], Loss: 0.3635\n",
      "Epoch [80/200], Loss: 0.3569\n",
      "Epoch [90/200], Loss: 0.3463\n",
      "Epoch [100/200], Loss: 0.3821\n",
      "Epoch [110/200], Loss: 0.3481\n",
      "Early stopping at epoch 116\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0164\n",
      "Epoch [20/200], Loss: 0.8971\n",
      "Epoch [30/200], Loss: 0.7219\n",
      "Epoch [40/200], Loss: 0.5685\n",
      "Epoch [50/200], Loss: 0.5059\n",
      "Epoch [60/200], Loss: 0.4805\n",
      "Epoch [70/200], Loss: 0.4028\n",
      "Epoch [80/200], Loss: 0.3767\n",
      "Early stopping at epoch 87\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0334\n",
      "Epoch [20/200], Loss: 0.8883\n",
      "Epoch [30/200], Loss: 0.7313\n",
      "Epoch [40/200], Loss: 0.6792\n",
      "Epoch [50/200], Loss: 0.5889\n",
      "Epoch [60/200], Loss: 0.5777\n",
      "Epoch [70/200], Loss: 0.4083\n",
      "Epoch [80/200], Loss: 0.4179\n",
      "Epoch [90/200], Loss: 0.3703\n",
      "Epoch [100/200], Loss: 0.3197\n",
      "Epoch [110/200], Loss: 0.3339\n",
      "Early stopping at epoch 113\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/200], Loss: 1.0261\n",
      "Epoch [20/200], Loss: 0.8363\n",
      "Epoch [30/200], Loss: 0.7742\n",
      "Epoch [40/200], Loss: 0.5981\n",
      "Epoch [50/200], Loss: 0.5684\n",
      "Epoch [60/200], Loss: 0.5549\n",
      "Epoch [70/200], Loss: 0.5111\n",
      "Epoch [80/200], Loss: 0.3879\n",
      "Epoch [90/200], Loss: 0.4159\n",
      "Early stopping at epoch 91\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0297\n",
      "Epoch [20/200], Loss: 0.9115\n",
      "Epoch [30/200], Loss: 0.6810\n",
      "Epoch [40/200], Loss: 0.5479\n",
      "Epoch [50/200], Loss: 0.5016\n",
      "Epoch [60/200], Loss: 0.5323\n",
      "Epoch [70/200], Loss: 0.5102\n",
      "Early stopping at epoch 72\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0123\n",
      "Epoch [20/200], Loss: 0.9009\n",
      "Epoch [30/200], Loss: 0.7131\n",
      "Epoch [40/200], Loss: 0.6029\n",
      "Epoch [50/200], Loss: 0.4661\n",
      "Epoch [60/200], Loss: 0.5415\n",
      "Epoch [70/200], Loss: 0.4534\n",
      "Epoch [80/200], Loss: 0.4076\n",
      "Epoch [90/200], Loss: 0.3876\n",
      "Early stopping at epoch 92\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0083\n",
      "Epoch [20/200], Loss: 0.9265\n",
      "Epoch [30/200], Loss: 0.7610\n",
      "Epoch [40/200], Loss: 0.6158\n",
      "Epoch [50/200], Loss: 0.6544\n",
      "Epoch [60/200], Loss: 0.4956\n",
      "Epoch [70/200], Loss: 0.4007\n",
      "Early stopping at epoch 72\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0333\n",
      "Epoch [20/200], Loss: 0.9389\n",
      "Epoch [30/200], Loss: 0.7733\n",
      "Epoch [40/200], Loss: 0.7195\n",
      "Epoch [50/200], Loss: 0.5555\n",
      "Epoch [60/200], Loss: 0.5036\n",
      "Epoch [70/200], Loss: 0.5173\n",
      "Epoch [80/200], Loss: 0.3678\n",
      "Epoch [90/200], Loss: 0.4051\n",
      "Early stopping at epoch 98\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0177\n",
      "Epoch [20/200], Loss: 0.8939\n",
      "Epoch [30/200], Loss: 0.7543\n",
      "Epoch [40/200], Loss: 0.6238\n",
      "Epoch [50/200], Loss: 0.5192\n",
      "Epoch [60/200], Loss: 0.5260\n",
      "Epoch [70/200], Loss: 0.4650\n",
      "Early stopping at epoch 74\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9767\n",
      "Epoch [20/200], Loss: 0.8665\n",
      "Epoch [30/200], Loss: 0.6652\n",
      "Epoch [40/200], Loss: 0.7179\n",
      "Epoch [50/200], Loss: 0.5789\n",
      "Epoch [60/200], Loss: 0.4901\n",
      "Epoch [70/200], Loss: 0.4585\n",
      "Early stopping at epoch 75\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0157\n",
      "Epoch [20/200], Loss: 0.8843\n",
      "Epoch [30/200], Loss: 0.7571\n",
      "Epoch [40/200], Loss: 0.5605\n",
      "Epoch [50/200], Loss: 0.5731\n",
      "Epoch [60/200], Loss: 0.4383\n",
      "Epoch [70/200], Loss: 0.4575\n",
      "Epoch [80/200], Loss: 0.3942\n",
      "Early stopping at epoch 84\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0034\n",
      "Epoch [20/200], Loss: 0.8798\n",
      "Epoch [30/200], Loss: 0.6982\n",
      "Epoch [40/200], Loss: 0.6156\n",
      "Epoch [50/200], Loss: 0.5638\n",
      "Early stopping at epoch 55\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0048\n",
      "Epoch [20/200], Loss: 0.8646\n",
      "Epoch [30/200], Loss: 0.6587\n",
      "Epoch [40/200], Loss: 0.5486\n",
      "Epoch [50/200], Loss: 0.5785\n",
      "Epoch [60/200], Loss: 0.3997\n",
      "Epoch [70/200], Loss: 0.4184\n",
      "Epoch [80/200], Loss: 0.3838\n",
      "Early stopping at epoch 88\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0098\n",
      "Epoch [20/200], Loss: 0.8819\n",
      "Epoch [30/200], Loss: 0.7226\n",
      "Epoch [40/200], Loss: 0.5732\n",
      "Epoch [50/200], Loss: 0.6142\n",
      "Epoch [60/200], Loss: 0.5096\n",
      "Epoch [70/200], Loss: 0.4399\n",
      "Early stopping at epoch 77\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0030\n",
      "Epoch [20/200], Loss: 0.8400\n",
      "Epoch [30/200], Loss: 0.7139\n",
      "Epoch [40/200], Loss: 0.6057\n",
      "Epoch [50/200], Loss: 0.5049\n",
      "Epoch [60/200], Loss: 0.4964\n",
      "Epoch [70/200], Loss: 0.5069\n",
      "Epoch [80/200], Loss: 0.4296\n",
      "Epoch [90/200], Loss: 0.4214\n",
      "Early stopping at epoch 97\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9884\n",
      "Epoch [20/200], Loss: 0.8423\n",
      "Epoch [30/200], Loss: 0.7430\n",
      "Epoch [40/200], Loss: 0.6064\n",
      "Epoch [50/200], Loss: 0.5865\n",
      "Epoch [60/200], Loss: 0.4766\n",
      "Epoch [70/200], Loss: 0.5344\n",
      "Epoch [80/200], Loss: 0.4632\n",
      "Epoch [90/200], Loss: 0.4846\n",
      "Early stopping at epoch 94\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0268\n",
      "Epoch [20/200], Loss: 0.9770\n",
      "Epoch [30/200], Loss: 0.8195\n",
      "Epoch [40/200], Loss: 0.7655\n",
      "Epoch [50/200], Loss: 0.6578\n",
      "Epoch [60/200], Loss: 0.5103\n",
      "Epoch [70/200], Loss: 0.5841\n",
      "Early stopping at epoch 71\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/200], Loss: 1.0031\n",
      "Epoch [20/200], Loss: 0.9128\n",
      "Epoch [30/200], Loss: 0.7800\n",
      "Epoch [40/200], Loss: 0.6129\n",
      "Epoch [50/200], Loss: 0.5265\n",
      "Epoch [60/200], Loss: 0.4764\n",
      "Epoch [70/200], Loss: 0.4293\n",
      "Early stopping at epoch 78\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0236\n",
      "Epoch [20/200], Loss: 0.8605\n",
      "Epoch [30/200], Loss: 0.7000\n",
      "Epoch [40/200], Loss: 0.6434\n",
      "Epoch [50/200], Loss: 0.5156\n",
      "Epoch [60/200], Loss: 0.4776\n",
      "Epoch [70/200], Loss: 0.3675\n",
      "Epoch [80/200], Loss: 0.4545\n",
      "Early stopping at epoch 89\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0304\n",
      "Epoch [20/200], Loss: 0.8197\n",
      "Epoch [30/200], Loss: 0.7015\n",
      "Epoch [40/200], Loss: 0.6653\n",
      "Epoch [50/200], Loss: 0.5858\n",
      "Epoch [60/200], Loss: 0.5023\n",
      "Epoch [70/200], Loss: 0.4356\n",
      "Epoch [80/200], Loss: 0.4275\n",
      "Early stopping at epoch 84\n",
      "Testing Time: 0.03 seconds\n",
      "Mean Training Time: 5.71 seconds\n",
      "Mean Test Accuracy: 0.7674\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)  # Apply dropout\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)  # Apply dropout\n",
    "        return x\n",
    "\n",
    "# Load the Pubmed dataset\n",
    "dataset = Planetoid(root='/tmp/Pubmed', name='Pubmed')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "window_size = 10\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 32, num_classes)  # Hidden layer size: 32 units\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= window_size:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zToQO-vt1fUy",
    "outputId": "8b8b37aa-7a51-428a-855c-c8db6f6749d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 0.9482\n",
      "Epoch [20/200], Loss: 0.6825\n",
      "Epoch [30/200], Loss: 0.4179\n",
      "Epoch [40/200], Loss: 0.2348\n",
      "Epoch [50/200], Loss: 0.1323\n",
      "Epoch [60/200], Loss: 0.0782\n",
      "Epoch [70/200], Loss: 0.0493\n",
      "Epoch [80/200], Loss: 0.0336\n",
      "Epoch [90/200], Loss: 0.0245\n",
      "Epoch [100/200], Loss: 0.0188\n",
      "Epoch [110/200], Loss: 0.0151\n",
      "Epoch [120/200], Loss: 0.0124\n",
      "Epoch [130/200], Loss: 0.0105\n",
      "Epoch [140/200], Loss: 0.0090\n",
      "Epoch [150/200], Loss: 0.0078\n",
      "Epoch [160/200], Loss: 0.0069\n",
      "Epoch [170/200], Loss: 0.0061\n",
      "Epoch [180/200], Loss: 0.0054\n",
      "Epoch [190/200], Loss: 0.0049\n",
      "Epoch [200/200], Loss: 0.0044\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9637\n",
      "Epoch [20/200], Loss: 0.7141\n",
      "Epoch [30/200], Loss: 0.4522\n",
      "Epoch [40/200], Loss: 0.2596\n",
      "Epoch [50/200], Loss: 0.1464\n",
      "Epoch [60/200], Loss: 0.0856\n",
      "Epoch [70/200], Loss: 0.0531\n",
      "Epoch [80/200], Loss: 0.0355\n",
      "Epoch [90/200], Loss: 0.0254\n",
      "Epoch [100/200], Loss: 0.0193\n",
      "Epoch [110/200], Loss: 0.0153\n",
      "Epoch [120/200], Loss: 0.0125\n",
      "Epoch [130/200], Loss: 0.0105\n",
      "Epoch [140/200], Loss: 0.0090\n",
      "Epoch [150/200], Loss: 0.0077\n",
      "Epoch [160/200], Loss: 0.0068\n",
      "Epoch [170/200], Loss: 0.0060\n",
      "Epoch [180/200], Loss: 0.0053\n",
      "Epoch [190/200], Loss: 0.0048\n",
      "Epoch [200/200], Loss: 0.0043\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9381\n",
      "Epoch [20/200], Loss: 0.6831\n",
      "Epoch [30/200], Loss: 0.4377\n",
      "Epoch [40/200], Loss: 0.2605\n",
      "Epoch [50/200], Loss: 0.1517\n",
      "Epoch [60/200], Loss: 0.0901\n",
      "Epoch [70/200], Loss: 0.0565\n",
      "Epoch [80/200], Loss: 0.0380\n",
      "Epoch [90/200], Loss: 0.0274\n",
      "Epoch [100/200], Loss: 0.0209\n",
      "Epoch [110/200], Loss: 0.0166\n",
      "Epoch [120/200], Loss: 0.0136\n",
      "Epoch [130/200], Loss: 0.0114\n",
      "Epoch [140/200], Loss: 0.0097\n",
      "Epoch [150/200], Loss: 0.0084\n",
      "Epoch [160/200], Loss: 0.0074\n",
      "Epoch [170/200], Loss: 0.0065\n",
      "Epoch [180/200], Loss: 0.0058\n",
      "Epoch [190/200], Loss: 0.0052\n",
      "Epoch [200/200], Loss: 0.0047\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9332\n",
      "Epoch [20/200], Loss: 0.6567\n",
      "Epoch [30/200], Loss: 0.3958\n",
      "Epoch [40/200], Loss: 0.2207\n",
      "Epoch [50/200], Loss: 0.1232\n",
      "Epoch [60/200], Loss: 0.0721\n",
      "Epoch [70/200], Loss: 0.0452\n",
      "Epoch [80/200], Loss: 0.0306\n",
      "Epoch [90/200], Loss: 0.0223\n",
      "Epoch [100/200], Loss: 0.0172\n",
      "Epoch [110/200], Loss: 0.0138\n",
      "Epoch [120/200], Loss: 0.0114\n",
      "Epoch [130/200], Loss: 0.0096\n",
      "Epoch [140/200], Loss: 0.0083\n",
      "Epoch [150/200], Loss: 0.0072\n",
      "Epoch [160/200], Loss: 0.0063\n",
      "Epoch [170/200], Loss: 0.0056\n",
      "Epoch [180/200], Loss: 0.0050\n",
      "Epoch [190/200], Loss: 0.0045\n",
      "Epoch [200/200], Loss: 0.0041\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9559\n",
      "Epoch [20/200], Loss: 0.6981\n",
      "Epoch [30/200], Loss: 0.4314\n",
      "Epoch [40/200], Loss: 0.2436\n",
      "Epoch [50/200], Loss: 0.1370\n",
      "Epoch [60/200], Loss: 0.0806\n",
      "Epoch [70/200], Loss: 0.0503\n",
      "Epoch [80/200], Loss: 0.0338\n",
      "Epoch [90/200], Loss: 0.0244\n",
      "Epoch [100/200], Loss: 0.0186\n",
      "Epoch [110/200], Loss: 0.0148\n",
      "Epoch [120/200], Loss: 0.0122\n",
      "Epoch [130/200], Loss: 0.0103\n",
      "Epoch [140/200], Loss: 0.0088\n",
      "Epoch [150/200], Loss: 0.0076\n",
      "Epoch [160/200], Loss: 0.0067\n",
      "Epoch [170/200], Loss: 0.0059\n",
      "Epoch [180/200], Loss: 0.0053\n",
      "Epoch [190/200], Loss: 0.0048\n",
      "Epoch [200/200], Loss: 0.0043\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9579\n",
      "Epoch [20/200], Loss: 0.7046\n",
      "Epoch [30/200], Loss: 0.4465\n",
      "Epoch [40/200], Loss: 0.2591\n",
      "Epoch [50/200], Loss: 0.1478\n",
      "Epoch [60/200], Loss: 0.0870\n",
      "Epoch [70/200], Loss: 0.0543\n",
      "Epoch [80/200], Loss: 0.0365\n",
      "Epoch [90/200], Loss: 0.0263\n",
      "Epoch [100/200], Loss: 0.0200\n",
      "Epoch [110/200], Loss: 0.0159\n",
      "Epoch [120/200], Loss: 0.0130\n",
      "Epoch [130/200], Loss: 0.0109\n",
      "Epoch [140/200], Loss: 0.0093\n",
      "Epoch [150/200], Loss: 0.0080\n",
      "Epoch [160/200], Loss: 0.0070\n",
      "Epoch [170/200], Loss: 0.0062\n",
      "Epoch [180/200], Loss: 0.0056\n",
      "Epoch [190/200], Loss: 0.0050\n",
      "Epoch [200/200], Loss: 0.0045\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/200], Loss: 0.9621\n",
      "Epoch [20/200], Loss: 0.6948\n",
      "Epoch [30/200], Loss: 0.4272\n",
      "Epoch [40/200], Loss: 0.2408\n",
      "Epoch [50/200], Loss: 0.1343\n",
      "Epoch [60/200], Loss: 0.0783\n",
      "Epoch [70/200], Loss: 0.0486\n",
      "Epoch [80/200], Loss: 0.0327\n",
      "Epoch [90/200], Loss: 0.0236\n",
      "Epoch [100/200], Loss: 0.0181\n",
      "Epoch [110/200], Loss: 0.0144\n",
      "Epoch [120/200], Loss: 0.0119\n",
      "Epoch [130/200], Loss: 0.0100\n",
      "Epoch [140/200], Loss: 0.0086\n",
      "Epoch [150/200], Loss: 0.0075\n",
      "Epoch [160/200], Loss: 0.0066\n",
      "Epoch [170/200], Loss: 0.0058\n",
      "Epoch [180/200], Loss: 0.0052\n",
      "Epoch [190/200], Loss: 0.0047\n",
      "Epoch [200/200], Loss: 0.0043\n",
      "Testing Time: 0.04 seconds\n",
      "Epoch [10/200], Loss: 0.9197\n",
      "Epoch [20/200], Loss: 0.6516\n",
      "Epoch [30/200], Loss: 0.4075\n",
      "Epoch [40/200], Loss: 0.2355\n",
      "Epoch [50/200], Loss: 0.1335\n",
      "Epoch [60/200], Loss: 0.0782\n",
      "Epoch [70/200], Loss: 0.0490\n",
      "Epoch [80/200], Loss: 0.0333\n",
      "Epoch [90/200], Loss: 0.0242\n",
      "Epoch [100/200], Loss: 0.0186\n",
      "Epoch [110/200], Loss: 0.0149\n",
      "Epoch [120/200], Loss: 0.0123\n",
      "Epoch [130/200], Loss: 0.0104\n",
      "Epoch [140/200], Loss: 0.0089\n",
      "Epoch [150/200], Loss: 0.0077\n",
      "Epoch [160/200], Loss: 0.0068\n",
      "Epoch [170/200], Loss: 0.0060\n",
      "Epoch [180/200], Loss: 0.0054\n",
      "Epoch [190/200], Loss: 0.0048\n",
      "Epoch [200/200], Loss: 0.0044\n",
      "Testing Time: 0.05 seconds\n",
      "Epoch [10/200], Loss: 0.9813\n",
      "Epoch [20/200], Loss: 0.7324\n",
      "Epoch [30/200], Loss: 0.4670\n",
      "Epoch [40/200], Loss: 0.2706\n",
      "Epoch [50/200], Loss: 0.1530\n",
      "Epoch [60/200], Loss: 0.0890\n",
      "Epoch [70/200], Loss: 0.0548\n",
      "Epoch [80/200], Loss: 0.0365\n",
      "Epoch [90/200], Loss: 0.0261\n",
      "Epoch [100/200], Loss: 0.0199\n",
      "Epoch [110/200], Loss: 0.0158\n",
      "Epoch [120/200], Loss: 0.0130\n",
      "Epoch [130/200], Loss: 0.0109\n",
      "Epoch [140/200], Loss: 0.0093\n",
      "Epoch [150/200], Loss: 0.0081\n",
      "Epoch [160/200], Loss: 0.0071\n",
      "Epoch [170/200], Loss: 0.0063\n",
      "Epoch [180/200], Loss: 0.0056\n",
      "Epoch [190/200], Loss: 0.0051\n",
      "Epoch [200/200], Loss: 0.0046\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9761\n",
      "Epoch [20/200], Loss: 0.7423\n",
      "Epoch [30/200], Loss: 0.4856\n",
      "Epoch [40/200], Loss: 0.2883\n",
      "Epoch [50/200], Loss: 0.1678\n",
      "Epoch [60/200], Loss: 0.1005\n",
      "Epoch [70/200], Loss: 0.0636\n",
      "Epoch [80/200], Loss: 0.0429\n",
      "Epoch [90/200], Loss: 0.0308\n",
      "Epoch [100/200], Loss: 0.0233\n",
      "Epoch [110/200], Loss: 0.0184\n",
      "Epoch [120/200], Loss: 0.0149\n",
      "Epoch [130/200], Loss: 0.0124\n",
      "Epoch [140/200], Loss: 0.0106\n",
      "Epoch [150/200], Loss: 0.0091\n",
      "Epoch [160/200], Loss: 0.0080\n",
      "Epoch [170/200], Loss: 0.0070\n",
      "Epoch [180/200], Loss: 0.0062\n",
      "Epoch [190/200], Loss: 0.0056\n",
      "Epoch [200/200], Loss: 0.0050\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9666\n",
      "Epoch [20/200], Loss: 0.7059\n",
      "Epoch [30/200], Loss: 0.4413\n",
      "Epoch [40/200], Loss: 0.2529\n",
      "Epoch [50/200], Loss: 0.1430\n",
      "Epoch [60/200], Loss: 0.0843\n",
      "Epoch [70/200], Loss: 0.0527\n",
      "Epoch [80/200], Loss: 0.0354\n",
      "Epoch [90/200], Loss: 0.0255\n",
      "Epoch [100/200], Loss: 0.0195\n",
      "Epoch [110/200], Loss: 0.0155\n",
      "Epoch [120/200], Loss: 0.0127\n",
      "Epoch [130/200], Loss: 0.0107\n",
      "Epoch [140/200], Loss: 0.0092\n",
      "Epoch [150/200], Loss: 0.0080\n",
      "Epoch [160/200], Loss: 0.0070\n",
      "Epoch [170/200], Loss: 0.0062\n",
      "Epoch [180/200], Loss: 0.0055\n",
      "Epoch [190/200], Loss: 0.0050\n",
      "Epoch [200/200], Loss: 0.0045\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9303\n",
      "Epoch [20/200], Loss: 0.6610\n",
      "Epoch [30/200], Loss: 0.4077\n",
      "Epoch [40/200], Loss: 0.2337\n",
      "Epoch [50/200], Loss: 0.1338\n",
      "Epoch [60/200], Loss: 0.0800\n",
      "Epoch [70/200], Loss: 0.0509\n",
      "Epoch [80/200], Loss: 0.0348\n",
      "Epoch [90/200], Loss: 0.0254\n",
      "Epoch [100/200], Loss: 0.0196\n",
      "Epoch [110/200], Loss: 0.0157\n",
      "Epoch [120/200], Loss: 0.0130\n",
      "Epoch [130/200], Loss: 0.0109\n",
      "Epoch [140/200], Loss: 0.0094\n",
      "Epoch [150/200], Loss: 0.0082\n",
      "Epoch [160/200], Loss: 0.0072\n",
      "Epoch [170/200], Loss: 0.0064\n",
      "Epoch [180/200], Loss: 0.0057\n",
      "Epoch [190/200], Loss: 0.0051\n",
      "Epoch [200/200], Loss: 0.0046\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9500\n",
      "Epoch [20/200], Loss: 0.6952\n",
      "Epoch [30/200], Loss: 0.4429\n",
      "Epoch [40/200], Loss: 0.2600\n",
      "Epoch [50/200], Loss: 0.1493\n",
      "Epoch [60/200], Loss: 0.0883\n",
      "Epoch [70/200], Loss: 0.0551\n",
      "Epoch [80/200], Loss: 0.0369\n",
      "Epoch [90/200], Loss: 0.0265\n",
      "Epoch [100/200], Loss: 0.0201\n",
      "Epoch [110/200], Loss: 0.0160\n",
      "Epoch [120/200], Loss: 0.0131\n",
      "Epoch [130/200], Loss: 0.0110\n",
      "Epoch [140/200], Loss: 0.0094\n",
      "Epoch [150/200], Loss: 0.0081\n",
      "Epoch [160/200], Loss: 0.0071\n",
      "Epoch [170/200], Loss: 0.0063\n",
      "Epoch [180/200], Loss: 0.0056\n",
      "Epoch [190/200], Loss: 0.0051\n",
      "Epoch [200/200], Loss: 0.0046\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9560\n",
      "Epoch [20/200], Loss: 0.6936\n",
      "Epoch [30/200], Loss: 0.4248\n",
      "Epoch [40/200], Loss: 0.2373\n",
      "Epoch [50/200], Loss: 0.1318\n",
      "Epoch [60/200], Loss: 0.0766\n",
      "Epoch [70/200], Loss: 0.0475\n",
      "Epoch [80/200], Loss: 0.0319\n",
      "Epoch [90/200], Loss: 0.0230\n",
      "Epoch [100/200], Loss: 0.0176\n",
      "Epoch [110/200], Loss: 0.0141\n",
      "Epoch [120/200], Loss: 0.0116\n",
      "Epoch [130/200], Loss: 0.0097\n",
      "Epoch [140/200], Loss: 0.0083\n",
      "Epoch [150/200], Loss: 0.0073\n",
      "Epoch [160/200], Loss: 0.0064\n",
      "Epoch [170/200], Loss: 0.0057\n",
      "Epoch [180/200], Loss: 0.0051\n",
      "Epoch [190/200], Loss: 0.0045\n",
      "Epoch [200/200], Loss: 0.0041\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9382\n",
      "Epoch [20/200], Loss: 0.6806\n",
      "Epoch [30/200], Loss: 0.4310\n",
      "Epoch [40/200], Loss: 0.2506\n",
      "Epoch [50/200], Loss: 0.1421\n",
      "Epoch [60/200], Loss: 0.0832\n",
      "Epoch [70/200], Loss: 0.0517\n",
      "Epoch [80/200], Loss: 0.0347\n",
      "Epoch [90/200], Loss: 0.0251\n",
      "Epoch [100/200], Loss: 0.0192\n",
      "Epoch [110/200], Loss: 0.0153\n",
      "Epoch [120/200], Loss: 0.0126\n",
      "Epoch [130/200], Loss: 0.0106\n",
      "Epoch [140/200], Loss: 0.0091\n",
      "Epoch [150/200], Loss: 0.0079\n",
      "Epoch [160/200], Loss: 0.0069\n",
      "Epoch [170/200], Loss: 0.0062\n",
      "Epoch [180/200], Loss: 0.0055\n",
      "Epoch [190/200], Loss: 0.0049\n",
      "Epoch [200/200], Loss: 0.0045\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 1.0021\n",
      "Epoch [20/200], Loss: 0.7809\n",
      "Epoch [30/200], Loss: 0.5125\n",
      "Epoch [40/200], Loss: 0.3013\n",
      "Epoch [50/200], Loss: 0.1723\n",
      "Epoch [60/200], Loss: 0.1013\n",
      "Epoch [70/200], Loss: 0.0628\n",
      "Epoch [80/200], Loss: 0.0416\n",
      "Epoch [90/200], Loss: 0.0296\n",
      "Epoch [100/200], Loss: 0.0223\n",
      "Epoch [110/200], Loss: 0.0176\n",
      "Epoch [120/200], Loss: 0.0143\n",
      "Epoch [130/200], Loss: 0.0119\n",
      "Epoch [140/200], Loss: 0.0102\n",
      "Epoch [150/200], Loss: 0.0088\n",
      "Epoch [160/200], Loss: 0.0077\n",
      "Epoch [170/200], Loss: 0.0068\n",
      "Epoch [180/200], Loss: 0.0060\n",
      "Epoch [190/200], Loss: 0.0054\n",
      "Epoch [200/200], Loss: 0.0049\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9660\n",
      "Epoch [20/200], Loss: 0.7229\n",
      "Epoch [30/200], Loss: 0.4729\n",
      "Epoch [40/200], Loss: 0.2837\n",
      "Epoch [50/200], Loss: 0.1650\n",
      "Epoch [60/200], Loss: 0.0975\n",
      "Epoch [70/200], Loss: 0.0607\n",
      "Epoch [80/200], Loss: 0.0405\n",
      "Epoch [90/200], Loss: 0.0290\n",
      "Epoch [100/200], Loss: 0.0220\n",
      "Epoch [110/200], Loss: 0.0174\n",
      "Epoch [120/200], Loss: 0.0142\n",
      "Epoch [130/200], Loss: 0.0119\n",
      "Epoch [140/200], Loss: 0.0101\n",
      "Epoch [150/200], Loss: 0.0087\n",
      "Epoch [160/200], Loss: 0.0076\n",
      "Epoch [170/200], Loss: 0.0068\n",
      "Epoch [180/200], Loss: 0.0060\n",
      "Epoch [190/200], Loss: 0.0054\n",
      "Epoch [200/200], Loss: 0.0049\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9579\n",
      "Epoch [20/200], Loss: 0.7143\n",
      "Epoch [30/200], Loss: 0.4679\n",
      "Epoch [40/200], Loss: 0.2817\n",
      "Epoch [50/200], Loss: 0.1641\n",
      "Epoch [60/200], Loss: 0.0971\n",
      "Epoch [70/200], Loss: 0.0604\n",
      "Epoch [80/200], Loss: 0.0402\n",
      "Epoch [90/200], Loss: 0.0287\n",
      "Epoch [100/200], Loss: 0.0217\n",
      "Epoch [110/200], Loss: 0.0172\n",
      "Epoch [120/200], Loss: 0.0140\n",
      "Epoch [130/200], Loss: 0.0117\n",
      "Epoch [140/200], Loss: 0.0100\n",
      "Epoch [150/200], Loss: 0.0086\n",
      "Epoch [160/200], Loss: 0.0076\n",
      "Epoch [170/200], Loss: 0.0067\n",
      "Epoch [180/200], Loss: 0.0060\n",
      "Epoch [190/200], Loss: 0.0053\n",
      "Epoch [200/200], Loss: 0.0048\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9483\n",
      "Epoch [20/200], Loss: 0.6937\n",
      "Epoch [30/200], Loss: 0.4399\n",
      "Epoch [40/200], Loss: 0.2576\n",
      "Epoch [50/200], Loss: 0.1500\n",
      "Epoch [60/200], Loss: 0.0908\n",
      "Epoch [70/200], Loss: 0.0581\n",
      "Epoch [80/200], Loss: 0.0397\n",
      "Epoch [90/200], Loss: 0.0289\n",
      "Epoch [100/200], Loss: 0.0221\n",
      "Epoch [110/200], Loss: 0.0176\n",
      "Epoch [120/200], Loss: 0.0145\n",
      "Epoch [130/200], Loss: 0.0122\n",
      "Epoch [140/200], Loss: 0.0104\n",
      "Epoch [150/200], Loss: 0.0090\n",
      "Epoch [160/200], Loss: 0.0079\n",
      "Epoch [170/200], Loss: 0.0070\n",
      "Epoch [180/200], Loss: 0.0062\n",
      "Epoch [190/200], Loss: 0.0056\n",
      "Epoch [200/200], Loss: 0.0051\n",
      "Testing Time: 0.03 seconds\n",
      "Epoch [10/200], Loss: 0.9262\n",
      "Epoch [20/200], Loss: 0.6568\n",
      "Epoch [30/200], Loss: 0.4105\n",
      "Epoch [40/200], Loss: 0.2355\n",
      "Epoch [50/200], Loss: 0.1313\n",
      "Epoch [60/200], Loss: 0.0755\n",
      "Epoch [70/200], Loss: 0.0465\n",
      "Epoch [80/200], Loss: 0.0311\n",
      "Epoch [90/200], Loss: 0.0224\n",
      "Epoch [100/200], Loss: 0.0171\n",
      "Epoch [110/200], Loss: 0.0136\n",
      "Epoch [120/200], Loss: 0.0112\n",
      "Epoch [130/200], Loss: 0.0094\n",
      "Epoch [140/200], Loss: 0.0081\n",
      "Epoch [150/200], Loss: 0.0070\n",
      "Epoch [160/200], Loss: 0.0061\n",
      "Epoch [170/200], Loss: 0.0054\n",
      "Epoch [180/200], Loss: 0.0048\n",
      "Epoch [190/200], Loss: 0.0043\n",
      "Epoch [200/200], Loss: 0.0039\n",
      "Testing Time: 0.03 seconds\n",
      "Mean Training Time: 12.68 seconds\n",
      "Mean Test Accuracy: 0.7634\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Load the Pubmed dataset\n",
    "dataset = Planetoid(root='/tmp/Pubmed', name='Pubmed')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "window_size = 10\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 32, num_classes)  # Hidden layer size: 32 units\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= window_size:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqGftg9t2O_o",
    "outputId": "954aa6a2-ac2a-4880-c66f-c8fac02c0298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 1.6229\n",
      "Epoch [20/200], Loss: 1.2539\n",
      "Epoch [30/200], Loss: 0.7972\n",
      "Epoch [40/200], Loss: 0.4255\n",
      "Epoch [50/200], Loss: 0.2164\n",
      "Epoch [60/200], Loss: 0.1179\n",
      "Epoch [70/200], Loss: 0.0718\n",
      "Epoch [80/200], Loss: 0.0485\n",
      "Epoch [90/200], Loss: 0.0355\n",
      "Epoch [100/200], Loss: 0.0274\n",
      "Epoch [110/200], Loss: 0.0220\n",
      "Epoch [120/200], Loss: 0.0181\n",
      "Epoch [130/200], Loss: 0.0153\n",
      "Epoch [140/200], Loss: 0.0131\n",
      "Epoch [150/200], Loss: 0.0114\n",
      "Epoch [160/200], Loss: 0.0100\n",
      "Epoch [170/200], Loss: 0.0089\n",
      "Epoch [180/200], Loss: 0.0079\n",
      "Epoch [190/200], Loss: 0.0071\n",
      "Epoch [200/200], Loss: 0.0064\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6196\n",
      "Epoch [20/200], Loss: 1.2403\n",
      "Epoch [30/200], Loss: 0.7664\n",
      "Epoch [40/200], Loss: 0.3954\n",
      "Epoch [50/200], Loss: 0.1987\n",
      "Epoch [60/200], Loss: 0.1083\n",
      "Epoch [70/200], Loss: 0.0659\n",
      "Epoch [80/200], Loss: 0.0442\n",
      "Epoch [90/200], Loss: 0.0320\n",
      "Epoch [100/200], Loss: 0.0245\n",
      "Epoch [110/200], Loss: 0.0195\n",
      "Epoch [120/200], Loss: 0.0161\n",
      "Epoch [130/200], Loss: 0.0135\n",
      "Epoch [140/200], Loss: 0.0116\n",
      "Epoch [150/200], Loss: 0.0101\n",
      "Epoch [160/200], Loss: 0.0088\n",
      "Epoch [170/200], Loss: 0.0078\n",
      "Epoch [180/200], Loss: 0.0070\n",
      "Epoch [190/200], Loss: 0.0063\n",
      "Epoch [200/200], Loss: 0.0057\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5873\n",
      "Epoch [20/200], Loss: 1.1754\n",
      "Epoch [30/200], Loss: 0.6860\n",
      "Epoch [40/200], Loss: 0.3324\n",
      "Epoch [50/200], Loss: 0.1615\n",
      "Epoch [60/200], Loss: 0.0882\n",
      "Epoch [70/200], Loss: 0.0547\n",
      "Epoch [80/200], Loss: 0.0374\n",
      "Epoch [90/200], Loss: 0.0276\n",
      "Epoch [100/200], Loss: 0.0214\n",
      "Epoch [110/200], Loss: 0.0173\n",
      "Epoch [120/200], Loss: 0.0144\n",
      "Epoch [130/200], Loss: 0.0122\n",
      "Epoch [140/200], Loss: 0.0105\n",
      "Epoch [150/200], Loss: 0.0091\n",
      "Epoch [160/200], Loss: 0.0081\n",
      "Epoch [170/200], Loss: 0.0072\n",
      "Epoch [180/200], Loss: 0.0064\n",
      "Epoch [190/200], Loss: 0.0058\n",
      "Epoch [200/200], Loss: 0.0053\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5909\n",
      "Epoch [20/200], Loss: 1.1889\n",
      "Epoch [30/200], Loss: 0.7163\n",
      "Epoch [40/200], Loss: 0.3629\n",
      "Epoch [50/200], Loss: 0.1825\n",
      "Epoch [60/200], Loss: 0.1012\n",
      "Epoch [70/200], Loss: 0.0629\n",
      "Epoch [80/200], Loss: 0.0429\n",
      "Epoch [90/200], Loss: 0.0314\n",
      "Epoch [100/200], Loss: 0.0242\n",
      "Epoch [110/200], Loss: 0.0194\n",
      "Epoch [120/200], Loss: 0.0160\n",
      "Epoch [130/200], Loss: 0.0135\n",
      "Epoch [140/200], Loss: 0.0116\n",
      "Epoch [150/200], Loss: 0.0100\n",
      "Epoch [160/200], Loss: 0.0088\n",
      "Epoch [170/200], Loss: 0.0078\n",
      "Epoch [180/200], Loss: 0.0070\n",
      "Epoch [190/200], Loss: 0.0063\n",
      "Epoch [200/200], Loss: 0.0057\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5861\n",
      "Epoch [20/200], Loss: 1.1789\n",
      "Epoch [30/200], Loss: 0.7028\n",
      "Epoch [40/200], Loss: 0.3535\n",
      "Epoch [50/200], Loss: 0.1786\n",
      "Epoch [60/200], Loss: 0.1004\n",
      "Epoch [70/200], Loss: 0.0631\n",
      "Epoch [80/200], Loss: 0.0435\n",
      "Epoch [90/200], Loss: 0.0321\n",
      "Epoch [100/200], Loss: 0.0249\n",
      "Epoch [110/200], Loss: 0.0200\n",
      "Epoch [120/200], Loss: 0.0165\n",
      "Epoch [130/200], Loss: 0.0139\n",
      "Epoch [140/200], Loss: 0.0119\n",
      "Epoch [150/200], Loss: 0.0104\n",
      "Epoch [160/200], Loss: 0.0091\n",
      "Epoch [170/200], Loss: 0.0081\n",
      "Epoch [180/200], Loss: 0.0072\n",
      "Epoch [190/200], Loss: 0.0065\n",
      "Epoch [200/200], Loss: 0.0059\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6135\n",
      "Epoch [20/200], Loss: 1.2286\n",
      "Epoch [30/200], Loss: 0.7576\n",
      "Epoch [40/200], Loss: 0.3900\n",
      "Epoch [50/200], Loss: 0.1941\n",
      "Epoch [60/200], Loss: 0.1043\n",
      "Epoch [70/200], Loss: 0.0627\n",
      "Epoch [80/200], Loss: 0.0419\n",
      "Epoch [90/200], Loss: 0.0303\n",
      "Epoch [100/200], Loss: 0.0232\n",
      "Epoch [110/200], Loss: 0.0185\n",
      "Epoch [120/200], Loss: 0.0152\n",
      "Epoch [130/200], Loss: 0.0128\n",
      "Epoch [140/200], Loss: 0.0109\n",
      "Epoch [150/200], Loss: 0.0095\n",
      "Epoch [160/200], Loss: 0.0083\n",
      "Epoch [170/200], Loss: 0.0074\n",
      "Epoch [180/200], Loss: 0.0066\n",
      "Epoch [190/200], Loss: 0.0059\n",
      "Epoch [200/200], Loss: 0.0054\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6035\n",
      "Epoch [20/200], Loss: 1.2310\n",
      "Epoch [30/200], Loss: 0.7749\n",
      "Epoch [40/200], Loss: 0.4093\n",
      "Epoch [50/200], Loss: 0.2078\n",
      "Epoch [60/200], Loss: 0.1138\n",
      "Epoch [70/200], Loss: 0.0696\n",
      "Epoch [80/200], Loss: 0.0469\n",
      "Epoch [90/200], Loss: 0.0341\n",
      "Epoch [100/200], Loss: 0.0262\n",
      "Epoch [110/200], Loss: 0.0209\n",
      "Epoch [120/200], Loss: 0.0172\n",
      "Epoch [130/200], Loss: 0.0145\n",
      "Epoch [140/200], Loss: 0.0124\n",
      "Epoch [150/200], Loss: 0.0107\n",
      "Epoch [160/200], Loss: 0.0094\n",
      "Epoch [170/200], Loss: 0.0084\n",
      "Epoch [180/200], Loss: 0.0075\n",
      "Epoch [190/200], Loss: 0.0067\n",
      "Epoch [200/200], Loss: 0.0061\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5793\n",
      "Epoch [20/200], Loss: 1.1565\n",
      "Epoch [30/200], Loss: 0.6817\n",
      "Epoch [40/200], Loss: 0.3477\n",
      "Epoch [50/200], Loss: 0.1796\n",
      "Epoch [60/200], Loss: 0.1017\n",
      "Epoch [70/200], Loss: 0.0638\n",
      "Epoch [80/200], Loss: 0.0437\n",
      "Epoch [90/200], Loss: 0.0320\n",
      "Epoch [100/200], Loss: 0.0247\n",
      "Epoch [110/200], Loss: 0.0197\n",
      "Epoch [120/200], Loss: 0.0162\n",
      "Epoch [130/200], Loss: 0.0136\n",
      "Epoch [140/200], Loss: 0.0116\n",
      "Epoch [150/200], Loss: 0.0101\n",
      "Epoch [160/200], Loss: 0.0088\n",
      "Epoch [170/200], Loss: 0.0078\n",
      "Epoch [180/200], Loss: 0.0070\n",
      "Epoch [190/200], Loss: 0.0063\n",
      "Epoch [200/200], Loss: 0.0057\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5806\n",
      "Epoch [20/200], Loss: 1.1694\n",
      "Epoch [30/200], Loss: 0.6946\n",
      "Epoch [40/200], Loss: 0.3482\n",
      "Epoch [50/200], Loss: 0.1740\n",
      "Epoch [60/200], Loss: 0.0964\n",
      "Epoch [70/200], Loss: 0.0599\n",
      "Epoch [80/200], Loss: 0.0410\n",
      "Epoch [90/200], Loss: 0.0301\n",
      "Epoch [100/200], Loss: 0.0234\n",
      "Epoch [110/200], Loss: 0.0188\n",
      "Epoch [120/200], Loss: 0.0156\n",
      "Epoch [130/200], Loss: 0.0132\n",
      "Epoch [140/200], Loss: 0.0114\n",
      "Epoch [150/200], Loss: 0.0099\n",
      "Epoch [160/200], Loss: 0.0087\n",
      "Epoch [170/200], Loss: 0.0078\n",
      "Epoch [180/200], Loss: 0.0070\n",
      "Epoch [190/200], Loss: 0.0063\n",
      "Epoch [200/200], Loss: 0.0057\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5936\n",
      "Epoch [20/200], Loss: 1.1920\n",
      "Epoch [30/200], Loss: 0.7235\n",
      "Epoch [40/200], Loss: 0.3749\n",
      "Epoch [50/200], Loss: 0.1938\n",
      "Epoch [60/200], Loss: 0.1094\n",
      "Epoch [70/200], Loss: 0.0684\n",
      "Epoch [80/200], Loss: 0.0468\n",
      "Epoch [90/200], Loss: 0.0342\n",
      "Epoch [100/200], Loss: 0.0264\n",
      "Epoch [110/200], Loss: 0.0211\n",
      "Epoch [120/200], Loss: 0.0173\n",
      "Epoch [130/200], Loss: 0.0146\n",
      "Epoch [140/200], Loss: 0.0125\n",
      "Epoch [150/200], Loss: 0.0108\n",
      "Epoch [160/200], Loss: 0.0095\n",
      "Epoch [170/200], Loss: 0.0084\n",
      "Epoch [180/200], Loss: 0.0075\n",
      "Epoch [190/200], Loss: 0.0067\n",
      "Epoch [200/200], Loss: 0.0061\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6040\n",
      "Epoch [20/200], Loss: 1.2077\n",
      "Epoch [30/200], Loss: 0.7272\n",
      "Epoch [40/200], Loss: 0.3658\n",
      "Epoch [50/200], Loss: 0.1822\n",
      "Epoch [60/200], Loss: 0.1003\n",
      "Epoch [70/200], Loss: 0.0620\n",
      "Epoch [80/200], Loss: 0.0423\n",
      "Epoch [90/200], Loss: 0.0310\n",
      "Epoch [100/200], Loss: 0.0239\n",
      "Epoch [110/200], Loss: 0.0192\n",
      "Epoch [120/200], Loss: 0.0159\n",
      "Epoch [130/200], Loss: 0.0134\n",
      "Epoch [140/200], Loss: 0.0115\n",
      "Epoch [150/200], Loss: 0.0100\n",
      "Epoch [160/200], Loss: 0.0088\n",
      "Epoch [170/200], Loss: 0.0079\n",
      "Epoch [180/200], Loss: 0.0070\n",
      "Epoch [190/200], Loss: 0.0063\n",
      "Epoch [200/200], Loss: 0.0057\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5937\n",
      "Epoch [20/200], Loss: 1.2077\n",
      "Epoch [30/200], Loss: 0.7541\n",
      "Epoch [40/200], Loss: 0.3979\n",
      "Epoch [50/200], Loss: 0.2016\n",
      "Epoch [60/200], Loss: 0.1097\n",
      "Epoch [70/200], Loss: 0.0663\n",
      "Epoch [80/200], Loss: 0.0441\n",
      "Epoch [90/200], Loss: 0.0316\n",
      "Epoch [100/200], Loss: 0.0240\n",
      "Epoch [110/200], Loss: 0.0190\n",
      "Epoch [120/200], Loss: 0.0155\n",
      "Epoch [130/200], Loss: 0.0130\n",
      "Epoch [140/200], Loss: 0.0111\n",
      "Epoch [150/200], Loss: 0.0096\n",
      "Epoch [160/200], Loss: 0.0084\n",
      "Epoch [170/200], Loss: 0.0074\n",
      "Epoch [180/200], Loss: 0.0066\n",
      "Epoch [190/200], Loss: 0.0059\n",
      "Epoch [200/200], Loss: 0.0054\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6197\n",
      "Epoch [20/200], Loss: 1.2542\n",
      "Epoch [30/200], Loss: 0.8020\n",
      "Epoch [40/200], Loss: 0.4338\n",
      "Epoch [50/200], Loss: 0.2263\n",
      "Epoch [60/200], Loss: 0.1267\n",
      "Epoch [70/200], Loss: 0.0783\n",
      "Epoch [80/200], Loss: 0.0528\n",
      "Epoch [90/200], Loss: 0.0381\n",
      "Epoch [100/200], Loss: 0.0290\n",
      "Epoch [110/200], Loss: 0.0230\n",
      "Epoch [120/200], Loss: 0.0187\n",
      "Epoch [130/200], Loss: 0.0157\n",
      "Epoch [140/200], Loss: 0.0134\n",
      "Epoch [150/200], Loss: 0.0115\n",
      "Epoch [160/200], Loss: 0.0101\n",
      "Epoch [170/200], Loss: 0.0089\n",
      "Epoch [180/200], Loss: 0.0079\n",
      "Epoch [190/200], Loss: 0.0071\n",
      "Epoch [200/200], Loss: 0.0064\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5947\n",
      "Epoch [20/200], Loss: 1.1986\n",
      "Epoch [30/200], Loss: 0.7314\n",
      "Epoch [40/200], Loss: 0.3789\n",
      "Epoch [50/200], Loss: 0.1937\n",
      "Epoch [60/200], Loss: 0.1081\n",
      "Epoch [70/200], Loss: 0.0675\n",
      "Epoch [80/200], Loss: 0.0462\n",
      "Epoch [90/200], Loss: 0.0339\n",
      "Epoch [100/200], Loss: 0.0261\n",
      "Epoch [110/200], Loss: 0.0210\n",
      "Epoch [120/200], Loss: 0.0173\n",
      "Epoch [130/200], Loss: 0.0145\n",
      "Epoch [140/200], Loss: 0.0124\n",
      "Epoch [150/200], Loss: 0.0108\n",
      "Epoch [160/200], Loss: 0.0095\n",
      "Epoch [170/200], Loss: 0.0084\n",
      "Epoch [180/200], Loss: 0.0075\n",
      "Epoch [190/200], Loss: 0.0067\n",
      "Epoch [200/200], Loss: 0.0061\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5946\n",
      "Epoch [20/200], Loss: 1.1788\n",
      "Epoch [30/200], Loss: 0.6840\n",
      "Epoch [40/200], Loss: 0.3328\n",
      "Epoch [50/200], Loss: 0.1637\n",
      "Epoch [60/200], Loss: 0.0899\n",
      "Epoch [70/200], Loss: 0.0557\n",
      "Epoch [80/200], Loss: 0.0382\n",
      "Epoch [90/200], Loss: 0.0282\n",
      "Epoch [100/200], Loss: 0.0219\n",
      "Epoch [110/200], Loss: 0.0177\n",
      "Epoch [120/200], Loss: 0.0147\n",
      "Epoch [130/200], Loss: 0.0124\n",
      "Epoch [140/200], Loss: 0.0107\n",
      "Epoch [150/200], Loss: 0.0093\n",
      "Epoch [160/200], Loss: 0.0082\n",
      "Epoch [170/200], Loss: 0.0073\n",
      "Epoch [180/200], Loss: 0.0066\n",
      "Epoch [190/200], Loss: 0.0059\n",
      "Epoch [200/200], Loss: 0.0054\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6069\n",
      "Epoch [20/200], Loss: 1.2345\n",
      "Epoch [30/200], Loss: 0.7809\n",
      "Epoch [40/200], Loss: 0.4176\n",
      "Epoch [50/200], Loss: 0.2152\n",
      "Epoch [60/200], Loss: 0.1197\n",
      "Epoch [70/200], Loss: 0.0740\n",
      "Epoch [80/200], Loss: 0.0501\n",
      "Epoch [90/200], Loss: 0.0364\n",
      "Epoch [100/200], Loss: 0.0279\n",
      "Epoch [110/200], Loss: 0.0223\n",
      "Epoch [120/200], Loss: 0.0183\n",
      "Epoch [130/200], Loss: 0.0153\n",
      "Epoch [140/200], Loss: 0.0131\n",
      "Epoch [150/200], Loss: 0.0114\n",
      "Epoch [160/200], Loss: 0.0100\n",
      "Epoch [170/200], Loss: 0.0088\n",
      "Epoch [180/200], Loss: 0.0079\n",
      "Epoch [190/200], Loss: 0.0071\n",
      "Epoch [200/200], Loss: 0.0064\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6164\n",
      "Epoch [20/200], Loss: 1.2236\n",
      "Epoch [30/200], Loss: 0.7360\n",
      "Epoch [40/200], Loss: 0.3634\n",
      "Epoch [50/200], Loss: 0.1767\n",
      "Epoch [60/200], Loss: 0.0953\n",
      "Epoch [70/200], Loss: 0.0580\n",
      "Epoch [80/200], Loss: 0.0391\n",
      "Epoch [90/200], Loss: 0.0284\n",
      "Epoch [100/200], Loss: 0.0219\n",
      "Epoch [110/200], Loss: 0.0176\n",
      "Epoch [120/200], Loss: 0.0145\n",
      "Epoch [130/200], Loss: 0.0123\n",
      "Epoch [140/200], Loss: 0.0106\n",
      "Epoch [150/200], Loss: 0.0092\n",
      "Epoch [160/200], Loss: 0.0081\n",
      "Epoch [170/200], Loss: 0.0072\n",
      "Epoch [180/200], Loss: 0.0064\n",
      "Epoch [190/200], Loss: 0.0058\n",
      "Epoch [200/200], Loss: 0.0053\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6198\n",
      "Epoch [20/200], Loss: 1.2518\n",
      "Epoch [30/200], Loss: 0.7820\n",
      "Epoch [40/200], Loss: 0.4039\n",
      "Epoch [50/200], Loss: 0.1993\n",
      "Epoch [60/200], Loss: 0.1063\n",
      "Epoch [70/200], Loss: 0.0639\n",
      "Epoch [80/200], Loss: 0.0427\n",
      "Epoch [90/200], Loss: 0.0309\n",
      "Epoch [100/200], Loss: 0.0237\n",
      "Epoch [110/200], Loss: 0.0189\n",
      "Epoch [120/200], Loss: 0.0156\n",
      "Epoch [130/200], Loss: 0.0131\n",
      "Epoch [140/200], Loss: 0.0112\n",
      "Epoch [150/200], Loss: 0.0098\n",
      "Epoch [160/200], Loss: 0.0086\n",
      "Epoch [170/200], Loss: 0.0076\n",
      "Epoch [180/200], Loss: 0.0068\n",
      "Epoch [190/200], Loss: 0.0061\n",
      "Epoch [200/200], Loss: 0.0056\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5943\n",
      "Epoch [20/200], Loss: 1.1895\n",
      "Epoch [30/200], Loss: 0.7038\n",
      "Epoch [40/200], Loss: 0.3488\n",
      "Epoch [50/200], Loss: 0.1730\n",
      "Epoch [60/200], Loss: 0.0951\n",
      "Epoch [70/200], Loss: 0.0585\n",
      "Epoch [80/200], Loss: 0.0397\n",
      "Epoch [90/200], Loss: 0.0290\n",
      "Epoch [100/200], Loss: 0.0223\n",
      "Epoch [110/200], Loss: 0.0179\n",
      "Epoch [120/200], Loss: 0.0148\n",
      "Epoch [130/200], Loss: 0.0125\n",
      "Epoch [140/200], Loss: 0.0108\n",
      "Epoch [150/200], Loss: 0.0094\n",
      "Epoch [160/200], Loss: 0.0082\n",
      "Epoch [170/200], Loss: 0.0073\n",
      "Epoch [180/200], Loss: 0.0066\n",
      "Epoch [190/200], Loss: 0.0059\n",
      "Epoch [200/200], Loss: 0.0054\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.5851\n",
      "Epoch [20/200], Loss: 1.1851\n",
      "Epoch [30/200], Loss: 0.7178\n",
      "Epoch [40/200], Loss: 0.3665\n",
      "Epoch [50/200], Loss: 0.1851\n",
      "Epoch [60/200], Loss: 0.1033\n",
      "Epoch [70/200], Loss: 0.0643\n",
      "Epoch [80/200], Loss: 0.0439\n",
      "Epoch [90/200], Loss: 0.0321\n",
      "Epoch [100/200], Loss: 0.0247\n",
      "Epoch [110/200], Loss: 0.0198\n",
      "Epoch [120/200], Loss: 0.0163\n",
      "Epoch [130/200], Loss: 0.0137\n",
      "Epoch [140/200], Loss: 0.0118\n",
      "Epoch [150/200], Loss: 0.0102\n",
      "Epoch [160/200], Loss: 0.0090\n",
      "Epoch [170/200], Loss: 0.0079\n",
      "Epoch [180/200], Loss: 0.0071\n",
      "Epoch [190/200], Loss: 0.0064\n",
      "Epoch [200/200], Loss: 0.0058\n",
      "Testing Time: 0.02 seconds\n",
      "Mean Training Time: 7.22 seconds\n",
      "Mean Test Accuracy: 0.6451\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Load the CiteSeer dataset\n",
    "dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "window_size = 10\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 32, num_classes)  # Hidden layer size: 32 units\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= window_size:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gtg5BEFm5aF1",
    "outputId": "afefbbc0-a17a-4025-d2c6-afa8cc58d057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 1.6734\n",
      "Epoch [20/200], Loss: 1.4880\n",
      "Epoch [30/200], Loss: 1.2701\n",
      "Epoch [40/200], Loss: 1.0561\n",
      "Epoch [50/200], Loss: 0.9012\n",
      "Epoch [60/200], Loss: 0.8966\n",
      "Epoch [70/200], Loss: 0.8217\n",
      "Epoch [80/200], Loss: 0.7440\n",
      "Early stopping at epoch 81\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.7039\n",
      "Epoch [20/200], Loss: 1.5330\n",
      "Epoch [30/200], Loss: 1.3180\n",
      "Epoch [40/200], Loss: 1.1427\n",
      "Epoch [50/200], Loss: 0.9925\n",
      "Epoch [60/200], Loss: 0.9304\n",
      "Epoch [70/200], Loss: 0.8163\n",
      "Epoch [80/200], Loss: 0.7927\n",
      "Early stopping at epoch 83\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.7153\n",
      "Epoch [20/200], Loss: 1.5842\n",
      "Epoch [30/200], Loss: 1.4615\n",
      "Epoch [40/200], Loss: 1.1405\n",
      "Epoch [50/200], Loss: 1.0393\n",
      "Epoch [60/200], Loss: 0.9574\n",
      "Epoch [70/200], Loss: 0.7892\n",
      "Epoch [80/200], Loss: 0.8241\n",
      "Epoch [90/200], Loss: 0.8006\n",
      "Early stopping at epoch 92\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6921\n",
      "Epoch [20/200], Loss: 1.5040\n",
      "Epoch [30/200], Loss: 1.3299\n",
      "Epoch [40/200], Loss: 1.1191\n",
      "Epoch [50/200], Loss: 1.1021\n",
      "Epoch [60/200], Loss: 0.8699\n",
      "Epoch [70/200], Loss: 0.8657\n",
      "Epoch [80/200], Loss: 0.6949\n",
      "Epoch [90/200], Loss: 0.7607\n",
      "Epoch [100/200], Loss: 0.6313\n",
      "Early stopping at epoch 102\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6923\n",
      "Epoch [20/200], Loss: 1.5100\n",
      "Epoch [30/200], Loss: 1.2844\n",
      "Epoch [40/200], Loss: 1.0470\n",
      "Epoch [50/200], Loss: 0.9182\n",
      "Epoch [60/200], Loss: 0.8537\n",
      "Epoch [70/200], Loss: 0.8733\n",
      "Early stopping at epoch 73\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.7143\n",
      "Epoch [20/200], Loss: 1.5557\n",
      "Epoch [30/200], Loss: 1.3192\n",
      "Epoch [40/200], Loss: 1.1137\n",
      "Epoch [50/200], Loss: 0.9922\n",
      "Epoch [60/200], Loss: 0.9406\n",
      "Epoch [70/200], Loss: 0.8414\n",
      "Early stopping at epoch 77\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6959\n",
      "Epoch [20/200], Loss: 1.5342\n",
      "Epoch [30/200], Loss: 1.2933\n",
      "Epoch [40/200], Loss: 1.1345\n",
      "Epoch [50/200], Loss: 0.9734\n",
      "Epoch [60/200], Loss: 0.9247\n",
      "Epoch [70/200], Loss: 0.7458\n",
      "Early stopping at epoch 72\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6626\n",
      "Epoch [20/200], Loss: 1.4848\n",
      "Epoch [30/200], Loss: 1.2916\n",
      "Epoch [40/200], Loss: 0.9751\n",
      "Epoch [50/200], Loss: 0.8646\n",
      "Epoch [60/200], Loss: 0.8458\n",
      "Epoch [70/200], Loss: 0.7773\n",
      "Early stopping at epoch 74\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6749\n",
      "Epoch [20/200], Loss: 1.4803\n",
      "Epoch [30/200], Loss: 1.2099\n",
      "Epoch [40/200], Loss: 1.0814\n",
      "Epoch [50/200], Loss: 0.9436\n",
      "Epoch [60/200], Loss: 0.8307\n",
      "Epoch [70/200], Loss: 0.8851\n",
      "Epoch [80/200], Loss: 0.7808\n",
      "Early stopping at epoch 83\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.7132\n",
      "Epoch [20/200], Loss: 1.5164\n",
      "Epoch [30/200], Loss: 1.3547\n",
      "Epoch [40/200], Loss: 1.1660\n",
      "Epoch [50/200], Loss: 1.0626\n",
      "Epoch [60/200], Loss: 0.7888\n",
      "Epoch [70/200], Loss: 0.7373\n",
      "Epoch [80/200], Loss: 0.8285\n",
      "Epoch [90/200], Loss: 0.7060\n",
      "Epoch [100/200], Loss: 0.8317\n",
      "Epoch [110/200], Loss: 0.6425\n",
      "Early stopping at epoch 113\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.7286\n",
      "Epoch [20/200], Loss: 1.6003\n",
      "Epoch [30/200], Loss: 1.4990\n",
      "Epoch [40/200], Loss: 1.2583\n",
      "Epoch [50/200], Loss: 1.0422\n",
      "Epoch [60/200], Loss: 0.9783\n",
      "Early stopping at epoch 68\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6747\n",
      "Epoch [20/200], Loss: 1.5211\n",
      "Epoch [30/200], Loss: 1.2115\n",
      "Epoch [40/200], Loss: 1.0817\n",
      "Epoch [50/200], Loss: 0.8493\n",
      "Epoch [60/200], Loss: 0.7529\n",
      "Epoch [70/200], Loss: 0.8324\n",
      "Early stopping at epoch 78\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6922\n",
      "Epoch [20/200], Loss: 1.4996\n",
      "Epoch [30/200], Loss: 1.3128\n",
      "Epoch [40/200], Loss: 0.9987\n",
      "Epoch [50/200], Loss: 0.9258\n",
      "Epoch [60/200], Loss: 0.8891\n",
      "Early stopping at epoch 64\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.7046\n",
      "Epoch [20/200], Loss: 1.5178\n",
      "Epoch [30/200], Loss: 1.2955\n",
      "Epoch [40/200], Loss: 1.1372\n",
      "Epoch [50/200], Loss: 0.9243\n",
      "Epoch [60/200], Loss: 0.9908\n",
      "Epoch [70/200], Loss: 0.8189\n",
      "Early stopping at epoch 78\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6906\n",
      "Epoch [20/200], Loss: 1.5401\n",
      "Epoch [30/200], Loss: 1.2444\n",
      "Epoch [40/200], Loss: 1.0751\n",
      "Epoch [50/200], Loss: 0.8939\n",
      "Epoch [60/200], Loss: 0.7818\n",
      "Epoch [70/200], Loss: 0.8825\n",
      "Epoch [80/200], Loss: 0.8391\n",
      "Early stopping at epoch 83\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6876\n",
      "Epoch [20/200], Loss: 1.5637\n",
      "Epoch [30/200], Loss: 1.3846\n",
      "Epoch [40/200], Loss: 1.0905\n",
      "Epoch [50/200], Loss: 0.9612\n",
      "Epoch [60/200], Loss: 0.8680\n",
      "Early stopping at epoch 64\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.7158\n",
      "Epoch [20/200], Loss: 1.5200\n",
      "Epoch [30/200], Loss: 1.3346\n",
      "Epoch [40/200], Loss: 1.1225\n",
      "Epoch [50/200], Loss: 0.8968\n",
      "Epoch [60/200], Loss: 0.8636\n",
      "Epoch [70/200], Loss: 0.8002\n",
      "Early stopping at epoch 75\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.7037\n",
      "Epoch [20/200], Loss: 1.5540\n",
      "Epoch [30/200], Loss: 1.2968\n",
      "Epoch [40/200], Loss: 0.9864\n",
      "Epoch [50/200], Loss: 0.9247\n",
      "Epoch [60/200], Loss: 0.8171\n",
      "Epoch [70/200], Loss: 0.7941\n",
      "Epoch [80/200], Loss: 0.7001\n",
      "Epoch [90/200], Loss: 0.6919\n",
      "Early stopping at epoch 99\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6909\n",
      "Epoch [20/200], Loss: 1.4925\n",
      "Epoch [30/200], Loss: 1.2814\n",
      "Epoch [40/200], Loss: 1.1856\n",
      "Epoch [50/200], Loss: 0.9734\n",
      "Epoch [60/200], Loss: 0.7835\n",
      "Epoch [70/200], Loss: 0.8150\n",
      "Epoch [80/200], Loss: 0.7552\n",
      "Epoch [90/200], Loss: 0.6948\n",
      "Early stopping at epoch 93\n",
      "Testing Time: 0.02 seconds\n",
      "Epoch [10/200], Loss: 1.6723\n",
      "Epoch [20/200], Loss: 1.4937\n",
      "Epoch [30/200], Loss: 1.1999\n",
      "Epoch [40/200], Loss: 1.0964\n",
      "Epoch [50/200], Loss: 0.9941\n",
      "Early stopping at epoch 58\n",
      "Testing Time: 0.02 seconds\n",
      "Mean Training Time: 2.98 seconds\n",
      "Mean Test Accuracy: 0.6815\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)  # Apply dropout\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)  # Apply dropout\n",
    "        return x\n",
    "\n",
    "# Load the Pubmed dataset\n",
    "dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare the data\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Normalize features\n",
    "x /= x.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "\n",
    "# Set the hyperparameters\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "window_size = 10\n",
    "\n",
    "# Initialize lists to store time and accuracy\n",
    "training_times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for _ in range(20):\n",
    "    # Build the model\n",
    "    model = GCN(num_features, 32, num_classes)  # Hidden layer size: 32 units\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= window_size:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    training_times.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    _, pred = model(x, edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Testing Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate mean training time and test accuracy\n",
    "mean_training_time = sum(training_times) / len(training_times)\n",
    "mean_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "\n",
    "print(f\"Mean Training Time: {mean_training_time:.2f} seconds\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
